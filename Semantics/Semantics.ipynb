{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet\n",
    "\n",
    "NLTK provides a useful WordNet interface to play with the WordNet data (included into the `nltk.corpus`). Let's see how to use it\n",
    "\n",
    "First we import the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `wn` object to get the synsets of a given word.\n",
    "\n",
    "For instance, thise are the synsets related to the word `dog`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dog.n.01'),\n",
       " Synset('frump.n.01'),\n",
       " Synset('dog.n.03'),\n",
       " Synset('cad.n.01'),\n",
       " Synset('frank.n.02'),\n",
       " Synset('pawl.n.01'),\n",
       " Synset('andiron.n.01'),\n",
       " Synset('chase.v.01')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much more than expected. In order to make sense of each sense, we can plot their definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('dog.n.01')\n",
      "a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds\n",
      " \n",
      "Synset('frump.n.01')\n",
      "a dull unattractive unpleasant girl or woman\n",
      " \n",
      "Synset('dog.n.03')\n",
      "informal term for a man\n",
      " \n",
      "Synset('cad.n.01')\n",
      "someone who is morally reprehensible\n",
      " \n",
      "Synset('frank.n.02')\n",
      "a smooth-textured sausage of minced beef or pork usually smoked; often served on a bread roll\n",
      " \n",
      "Synset('pawl.n.01')\n",
      "a hinged catch that fits into a notch of a ratchet to move a wheel forward or prevent it from moving backward\n",
      " \n",
      "Synset('andiron.n.01')\n",
      "metal supports for logs in a fireplace\n",
      " \n",
      "Synset('chase.v.01')\n",
      "go after with the intent to catch\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for synset in wn.synsets('dog'):\n",
    "    print synset\n",
    "    print synset.definition()\n",
    "    print \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or examples for each synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('dog.n.01')\n",
      "[u'the dog barked all night']\n",
      " \n",
      "Synset('frump.n.01')\n",
      "[u'she got a reputation as a frump', u\"she's a real dog\"]\n",
      " \n",
      "Synset('dog.n.03')\n",
      "[u'you lucky dog']\n",
      " \n",
      "Synset('cad.n.01')\n",
      "[u'you dirty dog']\n",
      " \n",
      "Synset('frank.n.02')\n",
      "[]\n",
      " \n",
      "Synset('pawl.n.01')\n",
      "[]\n",
      " \n",
      "Synset('andiron.n.01')\n",
      "[u'the andirons were too hot to touch']\n",
      " \n",
      "Synset('chase.v.01')\n",
      "[u'The policeman chased the mugger down the alley', u'the dog chased the rabbit']\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for synset in wn.synsets('dog'):\n",
    "    print synset\n",
    "    print synset.examples()\n",
    "    print \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or lemmas related to the synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('dog.n.01')\n",
      "[u'dog', u'domestic_dog', u'Canis_familiaris']\n",
      " \n",
      "Synset('frump.n.01')\n",
      "[u'frump', u'dog']\n",
      " \n",
      "Synset('dog.n.03')\n",
      "[u'dog']\n",
      " \n",
      "Synset('cad.n.01')\n",
      "[u'cad', u'bounder', u'blackguard', u'dog', u'hound', u'heel']\n",
      " \n",
      "Synset('frank.n.02')\n",
      "[u'frank', u'frankfurter', u'hotdog', u'hot_dog', u'dog', u'wiener', u'wienerwurst', u'weenie']\n",
      " \n",
      "Synset('pawl.n.01')\n",
      "[u'pawl', u'detent', u'click', u'dog']\n",
      " \n",
      "Synset('andiron.n.01')\n",
      "[u'andiron', u'firedog', u'dog', u'dog-iron']\n",
      " \n",
      "Synset('chase.v.01')\n",
      "[u'chase', u'chase_after', u'trail', u'tail', u'tag', u'give_chase', u'dog', u'go_after', u'track']\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for synset in wn.synsets('dog'):\n",
    "    print synset\n",
    "    print synset.lemma_names()\n",
    "    print \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cool feature of the NLTK WordNet corpus is that it gives access to the **Open Multilingual WordNet**.\n",
    "\n",
    "It is useful to, for instance, get the lemmas in another languages a given synset, through the function `lemma_names`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('dog.n.01')\n",
      "[u'can', u'perro']\n",
      " \n",
      "Synset('frump.n.01')\n",
      "[]\n",
      " \n",
      "Synset('dog.n.03')\n",
      "[]\n",
      " \n",
      "Synset('cad.n.01')\n",
      "[]\n",
      " \n",
      "Synset('frank.n.02')\n",
      "[u'frankfurt']\n",
      " \n",
      "Synset('pawl.n.01')\n",
      "[]\n",
      " \n",
      "Synset('andiron.n.01')\n",
      "[]\n",
      " \n",
      "Synset('chase.v.01')\n",
      "[u'rastrear']\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for synset in wn.synsets('dog'):\n",
    "    print synset\n",
    "    print synset.lemma_names(lang=\"spa\")\n",
    "    print \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on the first dog sysnet.\n",
    "\n",
    "We can access to its relationships (`hypernyms`, `hyponyms`, `holonyms`, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('canine.n.02'), Synset('domestic_animal.n.01')]\n",
      "[Synset('basenji.n.01'), Synset('corgi.n.01'), Synset('cur.n.01'), Synset('dalmatian.n.02'), Synset('great_pyrenees.n.01'), Synset('griffon.n.02'), Synset('hunting_dog.n.01'), Synset('lapdog.n.01'), Synset('leonberg.n.01'), Synset('mexican_hairless.n.01'), Synset('newfoundland.n.01'), Synset('pooch.n.01'), Synset('poodle.n.01'), Synset('pug.n.01'), Synset('puppy.n.01'), Synset('spitz.n.01'), Synset('toy_dog.n.01'), Synset('working_dog.n.01')]\n",
      "[Synset('canis.n.01'), Synset('pack.n.06')]\n"
     ]
    }
   ],
   "source": [
    "dog = wn.synset('dog.n.01')\n",
    "print dog.hypernyms()\n",
    "print dog.hyponyms()\n",
    "print dog.member_holonyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some relations are defined by WordNet only over Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('bad.a.01.bad')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good = wn.synset('good.a.01')\n",
    "good.lemmas()[0].antonyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK also has implemented the **path-based similarity** function that we explained in class by means of the function `path_similarity`. It returns a score denoting how similar two word senses are, based on the shortest path that connects the senses in the is-a (hypernym/hypnoym) taxonomy. A score of 1 represents identity i.e. comparing a sense with itself will return 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog = wn.synset('dog.n.01')\n",
    "cat = wn.synset('cat.n.01')\n",
    "hit = wn.synset('hit.v.01')\n",
    "slap = wn.synset('slap.v.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog.path_similarity(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit.path_similarity(slap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has also the IC-based similarity. To that end you have to load  an information content file from the `wordnet_ic` corpus and then use this information with the `res_similarity` function to compute the IC-based similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.911666509036577"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet_ic\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "dog.res_similarity(cat, brown_ic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you prefer it, you can also train you own IC dictionary from any corpus. This is very useful if you want to compute the similary between words based on some particular data that you have for a given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.204023991374843"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import genesis\n",
    "genesis_ic = wn.ic(genesis, False, 0.0)\n",
    "dog.res_similarity(cat, genesis_ic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PMI\n",
    "\n",
    "In addition to the thesaurus-based metrics, we can also create similarity functions based on Distributional algorithms; that is, words that appear in similar contexts are expected to be similar.\n",
    "\n",
    "In particular, in class we presented the Point-wise Mutual Information as a measure the set the similarity of two words based on their contexts. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find words that appear in the same context is actually quite easy by using NLTK's `Text.similar()` function. This function takes a word w, finds all contexts w1w w2, then finds all words w' that appear in the same context, i.e. w1w' w2. (You can find the implementation online at http://nltk.org/nltk/text.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man time day year car moment world family house country child boy\n",
      "state job way war girl place word work\n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())\n",
    "text.similar('woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `PMI` to compute similarities between words.\n",
    "\n",
    "To that end, I have defined a function that takes two words, a dictionary with the frequency of the words `unigram_freq` and another dictionary `bigram_freq` with the count of each pair of words in the corpus. \n",
    "\n",
    "With these two dicts we can compute the joint probability of each pair of words (calculated as the fraction of the number of times they appear together and the total frequency of pairs of words) and, finally, compute the PMI as the fraction of the joint probability and the product of the marginal probabilites of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi(word1, word2, unigram_freq, bigram_freq):\n",
    "    import math\n",
    "    marginal_word1 = float(unigram_freq[word1]) / sum(unigram_freq.values())\n",
    "    marginal_word2 = float(unigram_freq[word2]) / sum(unigram_freq.values())\n",
    "    joint_w1_w2 = float(bigram_freq[(word1, word2)])/sum(bigram_freq.values())\n",
    "    pmi = round(math.log(max(0.0005,joint_w1_w2/(marginal_word1*marginal_word2)),2),2)\n",
    "    return pmi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has a package `collocations` that makes quite easy to compute the count of each pair of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = nltk.collocations.BigramCollocationFinder.from_words(nltk.corpus.brown.words(categories='news'), window_size = 20)\n",
    "finder.apply_freq_filter(20)\n",
    "bigrams_freq = bigrams.ngram_fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `FreqDist` function (which we already knew) to compute the individual frequencies of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = nltk.FreqDist( nltk.corpus.brown.words(categories=\"news\"))\n",
    "unigrams_freq = {unigram:freq for unigram, freq in unigrams.items() if freq >= 20}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the defined `pmi` function to compute the PMI similarity of two words.\n",
    "Let us see some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmi(u\"day\", u\"night\", unigrams_freq, bigrams_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.4"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmi(u\"per\", u\"cent\", unigrams_freq, bigrams_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-10.97"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmi(u\"day\", u\"administration\", unigrams_freq, bigrams_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.67"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmi(u\"government\", u\"administration\", unigrams_freq, bigrams_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK also provides some useful classes inside the `collocations` package to automatically compute this PMI-based similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collocations are expressions of multiple words which commonly co-occur. For example, the top ten bigram collocations in Brown news corpus are listed below, as measured using Pointwise Mutual Information (by using the `bigram_measures.pmi` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'$1,500,000', u'Rhine-Westphalia'),\n",
       " (u'$1,800', u'cadet'),\n",
       " (u'$1,800', u'termination'),\n",
       " (u'$1.4', u'subsidies'),\n",
       " (u'$1.5', u'$12.7'),\n",
       " (u'$10,000-per-year', u'French-born'),\n",
       " (u'$10,000-per-year', u'Holders'),\n",
       " (u'$10,000-per-year', u\"d'hotel\"),\n",
       " (u'$10,000-per-year', u'maitre'),\n",
       " (u'$100,000', u'kidnapping')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(nltk.corpus.brown.words(categories='news'), window_size = 20)\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While these words are highly collocated, the expressions are also very infrequent. Therefore it is useful to apply filters, such as ignoring all bigrams which occur less than 20 times in the corpus and removing the stopwords:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'per', u'cent'),\n",
       " (u'United', u'States'),\n",
       " (u'Mantle', u'Maris'),\n",
       " (u'New', u'York'),\n",
       " (u'White', u'House'),\n",
       " (u'sales', u'tax'),\n",
       " (u'home', u'runs'),\n",
       " (u'President', u'Kennedy'),\n",
       " (u'Mrs.', u'Mrs.'),\n",
       " (u'Mrs.', u'Robert'),\n",
       " (u'Robert', u'Mrs.'),\n",
       " (u'Mrs.', u'Jr.'),\n",
       " (u'Mrs.', u'William'),\n",
       " (u'Jr.', u'Mrs.'),\n",
       " (u'last', u'week'),\n",
       " (u'last', u'night'),\n",
       " (u'Mrs.', u'John'),\n",
       " (u'last', u'year'),\n",
       " (u'Mr.', u'Mrs.'),\n",
       " (u'John', u'Mrs.'),\n",
       " (u'Mr.', u'Mr.'),\n",
       " (u'Mrs.', u'Mr.'),\n",
       " (u'said', u'would'),\n",
       " (u'would', u'would'),\n",
       " (u'Mr.', u'said')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder.apply_freq_filter(20)\n",
    "ignored_words = nltk.corpus.stopwords.words('english')\n",
    "finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)\n",
    "finder.nbest(bigram_measures.pmi, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec\n",
    "\n",
    "Using Wor2vec in Python is in fact quite straightforward thanks to the package `gensim` (https://radimrehurek.com/gensim/), which has a package focused on Word2vec where you can create your own embeddings from a dataset.\n",
    "\n",
    "For more information on the generation of embeddings with this package, you can follow this tutorial: http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/#.WszQTXVuZhE\n",
    "\n",
    "The following code creates two embeddings model, one for the brown corpus and one for the movie_reviews dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import brown, movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Word2Vec(brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = Word2Vec(movie_reviews.sents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once trained the models, we can compute similarities between words. Try different words and check the differences between the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'woman', 0.8780252933502197),\n",
       " (u'girl', 0.8740298748016357),\n",
       " (u'boy', 0.8312857747077942),\n",
       " (u'young', 0.7791036367416382),\n",
       " (u'child', 0.7733349800109863)]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.wv.most_similar('man', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'woman', 0.8990576267242432),\n",
       " (u'girl', 0.8326300382614136),\n",
       " (u'boy', 0.8317219018936157),\n",
       " (u'child', 0.7928438186645508),\n",
       " (u'killer', 0.7519564032554626)]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mr.wv.most_similar('man', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Queen', 0.959496259689331),\n",
       " (u'Faith', 0.9590216875076294),\n",
       " (u'seasonal', 0.9546026587486267),\n",
       " (u\"town's\", 0.9513021111488342),\n",
       " (u'boom', 0.9510686993598938)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.wv.most_similar('movie', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'film', 0.9492945075035095),\n",
       " (u'picture', 0.8660645484924316),\n",
       " (u'sequel', 0.785641074180603),\n",
       " (u'case', 0.7478125095367432),\n",
       " (u'thing', 0.6979601383209229)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mr.wv.most_similar('movie', topn=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
