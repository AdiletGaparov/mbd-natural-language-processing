{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Sentiment Analysis refers to the use of text analysis and natural language processing to identify and extract subjective information in textual contents.\n",
    "\n",
    "In this practice we will focus on the analysis of the sentiment of a collection of tweets, applying some of the ideas that we have explored in class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "This corpus of tweets, developed by Sanford’s Natural Language processing research group\n",
    "The training set is collected by querying Twitter API for happy emoticons like \":)\" and sad emoticons like \":(\" and labelling them positive or negative. The emoticons were then stripped and Re-Tweets and duplicates removed.\n",
    "\n",
    "The data is a CSV with emoticons removed. Data file format has 6 fields:\n",
    "\n",
    "    0 - the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive) **Note**: For the dataset there is only negative and positive tweets\n",
    "    1 - the id of the tweet (2087)\n",
    "    2 - the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "    3 - the query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "    4 - the user that tweeted (robotickilldozr)\n",
    "    5 - the text of the tweet (Lyx is cool)\n",
    "\n",
    "It also contains around 500 tweets manually collected and labelled for testing purposes.\n",
    "\n",
    "We randomly sample and use 5000 tweets from this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import csv\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "def loadDataset(in_file):\n",
    "    my_path = os.getcwd()\n",
    "    path = os.path.join(my_path, in_file)\n",
    "    column_names = ['sentiment','ID', 'Date', 'Query', 'user_id', 'tweet']\n",
    "    tweets = pd.read_csv(path, delimiter=',', quotechar='\"', header= None, names= column_names, encoding=\"ISO-8859-1\")\n",
    "\n",
    "    print('Readed ', len(tweets), \"tweets\")\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data and check the number of positive and negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readed  1600000 tweets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "0    800000\n",
       "4    800000\n",
       "Name: ID, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_training_data = loadDataset(\"stanford_dataset/training.1600000.processed.noemoticon.csv\")\n",
    "raw_training_data.groupby('sentiment')['ID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset contains more than a million tweets, for our practice we will only use a sample of 5000 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Query</th>\n",
       "      <th>user_id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>909375</td>\n",
       "      <td>4</td>\n",
       "      <td>1751211417</td>\n",
       "      <td>Sat May 09 18:55:28 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Gen22</td>\n",
       "      <td>@Late2thePartee feeling any better hon?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>672967</td>\n",
       "      <td>0</td>\n",
       "      <td>2247338749</td>\n",
       "      <td>Fri Jun 19 18:57:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>kaate</td>\n",
       "      <td>@rndspringer: hope little bunny makes it!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210326</td>\n",
       "      <td>0</td>\n",
       "      <td>1974214603</td>\n",
       "      <td>Sat May 30 12:40:18 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>runfromzelda</td>\n",
       "      <td>I am such a failure. I have done no revision w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>597719</td>\n",
       "      <td>0</td>\n",
       "      <td>2219339532</td>\n",
       "      <td>Thu Jun 18 00:19:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tesskwan</td>\n",
       "      <td>byeee double math, i think i can't study that,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>885297</td>\n",
       "      <td>4</td>\n",
       "      <td>1686467866</td>\n",
       "      <td>Sun May 03 05:33:47 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RedNGreen</td>\n",
       "      <td>@johndobbs It sure kept a lot of bikers away f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment          ID                          Date     Query  \\\n",
       "909375          4  1751211417  Sat May 09 18:55:28 PDT 2009  NO_QUERY   \n",
       "672967          0  2247338749  Fri Jun 19 18:57:03 PDT 2009  NO_QUERY   \n",
       "210326          0  1974214603  Sat May 30 12:40:18 PDT 2009  NO_QUERY   \n",
       "597719          0  2219339532  Thu Jun 18 00:19:00 PDT 2009  NO_QUERY   \n",
       "885297          4  1686467866  Sun May 03 05:33:47 PDT 2009  NO_QUERY   \n",
       "\n",
       "             user_id                                              tweet  \n",
       "909375         Gen22           @Late2thePartee feeling any better hon?   \n",
       "672967         kaate         @rndspringer: hope little bunny makes it!   \n",
       "210326  runfromzelda  I am such a failure. I have done no revision w...  \n",
       "597719      tesskwan  byeee double math, i think i can't study that,...  \n",
       "885297     RedNGreen  @johndobbs It sure kept a lot of bikers away f...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample 5000 tweets from the dataset\n",
    "training_data = raw_training_data.sample(n=5000)\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the distribution of positive and negative tweets remains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "0    2485\n",
       "4    2515\n",
       "Name: ID, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.groupby('sentiment')['ID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate the interpretation of the results we are going to recode the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Query</th>\n",
       "      <th>user_id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>909375</td>\n",
       "      <td>positive</td>\n",
       "      <td>1751211417</td>\n",
       "      <td>Sat May 09 18:55:28 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Gen22</td>\n",
       "      <td>@Late2thePartee feeling any better hon?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>672967</td>\n",
       "      <td>negative</td>\n",
       "      <td>2247338749</td>\n",
       "      <td>Fri Jun 19 18:57:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>kaate</td>\n",
       "      <td>@rndspringer: hope little bunny makes it!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210326</td>\n",
       "      <td>negative</td>\n",
       "      <td>1974214603</td>\n",
       "      <td>Sat May 30 12:40:18 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>runfromzelda</td>\n",
       "      <td>I am such a failure. I have done no revision w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>597719</td>\n",
       "      <td>negative</td>\n",
       "      <td>2219339532</td>\n",
       "      <td>Thu Jun 18 00:19:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tesskwan</td>\n",
       "      <td>byeee double math, i think i can't study that,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>885297</td>\n",
       "      <td>positive</td>\n",
       "      <td>1686467866</td>\n",
       "      <td>Sun May 03 05:33:47 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RedNGreen</td>\n",
       "      <td>@johndobbs It sure kept a lot of bikers away f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment          ID                          Date     Query  \\\n",
       "909375  positive  1751211417  Sat May 09 18:55:28 PDT 2009  NO_QUERY   \n",
       "672967  negative  2247338749  Fri Jun 19 18:57:03 PDT 2009  NO_QUERY   \n",
       "210326  negative  1974214603  Sat May 30 12:40:18 PDT 2009  NO_QUERY   \n",
       "597719  negative  2219339532  Thu Jun 18 00:19:00 PDT 2009  NO_QUERY   \n",
       "885297  positive  1686467866  Sun May 03 05:33:47 PDT 2009  NO_QUERY   \n",
       "\n",
       "             user_id                                              tweet  \n",
       "909375         Gen22           @Late2thePartee feeling any better hon?   \n",
       "672967         kaate         @rndspringer: hope little bunny makes it!   \n",
       "210326  runfromzelda  I am such a failure. I have done no revision w...  \n",
       "597719      tesskwan  byeee double math, i think i can't study that,...  \n",
       "885297     RedNGreen  @johndobbs It sure kept a lot of bikers away f...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def recode_sentiment(series):\n",
    "    if series == 4:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'\n",
    "    \n",
    "training_data['sentiment'] = training_data['sentiment'].apply(recode_sentiment)\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Preprocessing\n",
    "\n",
    "At this step, we will preprocess the text in the tweets, tokenize and stem it. We will have to take care of specific markups (e.g., hashtags) related to Twitter, as well as of other aspects related to the sentiment analysis, like, for instance, emoticons.\n",
    "\n",
    "In the following, I give you an example of processing. I will use regular expressions to detect hashtags and change the detected hashtag by an indicator of the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashtags\n",
    "\n",
    "A hashtag is a word or an un-spaced phrase prefixed with the hash symbol (#). These are used to both naming subjects and phrases that are currently in trending topics. For example, #iPad, #news\n",
    "\n",
    "    Regular Expression: #(\\w+)\n",
    "\n",
    "    Replace Expression: HASH_hashtag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "hash_regex = re.compile(r\"#(\\w+)\")\n",
    "\n",
    "def hash_repl(match):\n",
    "    \"\"\"\n",
    "    Detect hashtags and create a new feature: _HASH_+text of the hashtag\n",
    "    \"\"\"\n",
    "    return '__HASH_'+match.group(1).upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this function, we will make use of the `re.sub` function. This function takes a regular expression (`hash_regex` in our case) and a replacing function (our `hash_repl` function) to change every appearance of the regular expression to the output of the replacing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happy midsummer everyone! My little brother has a bd today and here are few relatives having a dinner.. not so sure is it very nice and __HASH_HASHTAG'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the created function\n",
    "re.sub( hash_regex, hash_repl, 'happy midsummer everyone! My little brother has a bd today and here are few relatives having a dinner.. not so sure is it very nice and #hashtag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: More pre-processing\n",
    "\n",
    "Following the previous example, create more regex and functions to detect some other twitter-related aspects (e.g., user names, URLs, emoticons, punctuations, repetitions, stemming, ...)\n",
    "\n",
    "You may find interesting ideas in this regard in the following links:\n",
    " - Christopher Potts sentiment tokenizer: http://sentiment.christopherpotts.net/code-data/happyfuntokenizing.py\n",
    " - Brendan O’Connor twitter tokenizer: https://github.com/brendano/tweetmotif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to facilitate the application of these pre-processing steps, we will create a function to enclose all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function that encloses all the processing procedures\n",
    "def processAll(text):\n",
    "    \n",
    "    text = re.sub( hash_regex, hash_repl, text )\n",
    "    # All your pre-processing steps here\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new column in our dataframe with the processed text by applying our `processAll` function to all the text column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_data['processed_tweet'] = training_data.tweet.apply(processAll)\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Creation\n",
    "\n",
    "A wide variety of features can be used to build a classifier for tweets. The most widely used and basic feature set is word n-grams. However, there's a lot of domain specific information present in tweets that can also be used for classifying them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigrams\n",
    "\n",
    "Unigrams are the simplest features that can be used for text classification. A Tweet can be represented by a multiset of words present in it. We, however, have used the presence of unigrams in a tweet as a feature set. Presence of a word is more important than how many times it is repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'Example': 1,\n",
       "          'as': 1,\n",
       "          'of': 1,\n",
       "          'represented': 1,\n",
       "          'tweet': 1,\n",
       "          'unigrams': 1})"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"Example\", \"of\", \"tweet\", \"represented\", \"as\", \"unigrams\"]\n",
    "\n",
    "unigrams_fd = nltk.FreqDist()\n",
    "unigrams_fd.update(text)\n",
    "unigrams_fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "\n",
    "N-gram refers to an n-long sequence of words. Probabilistic Language Models based on Unigrams, Bigrams and Trigrams can be successfully used to predict the next word given a current context of words. In the domain of sentiment analysis, the performance of N-grams is unclear.\n",
    "\n",
    "As the order of the n-grams increases, they tend to be more and more sparse. Let's then try bi-gram and tri-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'Example,of': 1,\n",
       "          'as,unigrams': 1,\n",
       "          'of,tweet': 1,\n",
       "          'represented,as': 1,\n",
       "          'tweet,represented': 1})"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bigrams\n",
    "words_bi  = [ ','.join(map(str,bg)) for bg in nltk.bigrams(text) ]\n",
    "bi_grams_fd = nltk.FreqDist()\n",
    "bi_grams_fd.update( words_bi )\n",
    "bi_grams_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'Example,of,tweet': 1,\n",
       "          'of,tweet,represented': 1,\n",
       "          'represented,as,unigrams': 1,\n",
       "          'tweet,represented,as': 1})"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trigrams\n",
    "words_tri  = [ ','.join(map(str,tg)) for tg in nltk.trigrams(text) ]\n",
    "tri_grams_fd = nltk.FreqDist()\n",
    "tri_grams_fd.update( words_tri )\n",
    "tri_grams_fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the bigrams and trigrams models for the processed text in the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Wrapper function that encloses all the n-grams procedures\n",
    "\n",
    "def get_word_features(words):\n",
    "    bag = {}\n",
    "    words_uni = [ 'has(%s)'% ug for ug in words ]\n",
    "    words_bi  = [ 'has(%s)'% ','.join(map(str,bg)) for bg in nltk.bigrams(words) ]\n",
    "    words_tri = [ 'has(%s)'% ','.join(map(str,tg)) for tg in nltk.trigrams(words) ]\n",
    "    \n",
    "    for f in words_uni+words_bi+words_tri:\n",
    "        bag[f] = 1\n",
    "\n",
    "    return bag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negations\n",
    "\n",
    "The need negation detection in sentiment analysis can be illustrated by the difference in the meaning of the phrases, \"This is good\" vs. \"This is not good\" However, the negations occurring in natural language are seldom so simple. Handling the negation consists of two tasks – Detection of explicit negation cues and the scope of negation of these words.\n",
    "\n",
    "**Scope of Negation**\n",
    "\n",
    "Words immediately preceding and following the negation cues are the most negative and the words that come farther away do not lie in the scope of negation of such cues. We define left and right negativity of a word as the chances that meaning of that word is actually the opposite. Left negativity depends on the closest negation cue on the left and similarly for Right negativity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "negtn_regex = re.compile( r\"\"\"(?:\n",
    "    ^(?:never|no|nothing|nowhere|noone|none|not|\n",
    "        havent|hasnt|hadnt|cant|couldnt|shouldnt|\n",
    "        wont|wouldnt|dont|doesnt|didnt|isnt|arent|aint\n",
    "    )$\n",
    ")\n",
    "|\n",
    "n't\n",
    "\"\"\", re.X)\n",
    "\n",
    "def get_negation_features(words):\n",
    "    INF = 0.0\n",
    "    negtn = [ bool(negtn_regex.search(w)) for w in words ]\n",
    "\n",
    "    left = [0.0] * len(words)\n",
    "    prev = 0.0\n",
    "    for i in range(0,len(words)):\n",
    "        if( negtn[i] ):\n",
    "            prev = 1.0\n",
    "        left[i] = prev\n",
    "        prev = max( 0.0, prev-0.1)\n",
    "\n",
    "    right = [0.0] * len(words)\n",
    "    prev = 0.0\n",
    "    for i in reversed(range(0,len(words))):\n",
    "        if( negtn[i] ):\n",
    "            prev = 1.0\n",
    "        right[i] = prev\n",
    "        prev = max( 0.0, prev-0.1)\n",
    "\n",
    "    return dict( zip(\n",
    "                    ['neg_l('+w+')' for w in  words] + ['neg_r('+w+')' for w in  words],\n",
    "                    left + right ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg_l(This)': 0.0,\n",
       " 'neg_l(a)': 0.8,\n",
       " 'neg_l(does)': 0.0,\n",
       " 'neg_l(have)': 0.9,\n",
       " 'neg_l(negation)': 0.7000000000000001,\n",
       " 'neg_l(not)': 1.0,\n",
       " 'neg_l(text)': 0.0,\n",
       " 'neg_r(This)': 0.7000000000000001,\n",
       " 'neg_r(a)': 0.0,\n",
       " 'neg_r(does)': 0.9,\n",
       " 'neg_r(have)': 0.0,\n",
       " 'neg_r(negation)': 0.0,\n",
       " 'neg_r(not)': 1.0,\n",
       " 'neg_r(text)': 0.8}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "text = [\"This\",\"text\", \"does\", \"not\", \"have\", \"a\", \"negation\"]\n",
    "get_negation_features(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging\n",
    "\n",
    "With POS Tagging we can get the category of each word. Some of these categories are more interesting in order to infer the sentiment of given tweet. For example, adjectives are expected to carry most sentiment information than adverbs. In a similar way, some particular names can carry a positive or negative implication for particular domains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_features(words):\n",
    "    tags = {}\n",
    "    tagged_words = [ 'has(%s)'% w+'_'+tag for w,tag in nltk.pos_tag(words)]\n",
    "    \n",
    "    for tw in tagged_words:\n",
    "        tags[tw] = 1\n",
    "\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous step, let's create an function to apply all these creation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Wrapper function for the extraction of features\n",
    "def extract_features(text):\n",
    "    features = {}\n",
    "    \n",
    "    words = processAll(text)\n",
    "\n",
    "    word_features = get_word_features(words)\n",
    "    features.update( word_features )\n",
    "\n",
    "    negation_features = get_negation_features(words)\n",
    "    features.update( negation_features )\n",
    "    \n",
    "    pos_features = get_pos_features(words)\n",
    "    features.update( pos_features )\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>processed_tweet_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1258531</th>\n",
       "      <td>showing my mum twitter.</td>\n",
       "      <td>{u'neg_l(show)': 0.0, u'has(twitter)': 1, u'ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186787</th>\n",
       "      <td>Man I got to get up in 3 hours wow</td>\n",
       "      <td>{u'has(got,get,hour)': 1, u'neg_r(get)': 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859427</th>\n",
       "      <td>http://twitpic.com/4du9t - Playin in the H wit...</td>\n",
       "      <td>{u'has(hand)': 1, u'has(gonna)': 1, u'has(the,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301539</th>\n",
       "      <td>Mr. Adams is teaching us how to shake hands.</td>\n",
       "      <td>{u'has(hand)': 1, u'has(shake,hand)': 1, u'neg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89345</th>\n",
       "      <td>Sooo bored on this ride! My ipod died</td>\n",
       "      <td>{u'has(__punc_excl)': 1, u'has(ride,__punc_exc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     tweet  \\\n",
       "1258531                           showing my mum twitter.    \n",
       "186787                 Man I got to get up in 3 hours wow    \n",
       "859427   http://twitpic.com/4du9t - Playin in the H wit...   \n",
       "1301539      Mr. Adams is teaching us how to shake hands.    \n",
       "89345               Sooo bored on this ride! My ipod died    \n",
       "\n",
       "                                  processed_tweet_features  \n",
       "1258531  {u'neg_l(show)': 0.0, u'has(twitter)': 1, u'ha...  \n",
       "186787   {u'has(got,get,hour)': 1, u'neg_r(get)': 0.0, ...  \n",
       "859427   {u'has(hand)': 1, u'has(gonna)': 1, u'has(the,...  \n",
       "1301539  {u'has(hand)': 1, u'has(shake,hand)': 1, u'neg...  \n",
       "89345    {u'has(__punc_excl)': 1, u'has(ride,__punc_exc...  "
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['processed_tweet_features'] = training_data.tweet.apply(extract_features)\n",
    "training_data[['tweet','processed_tweet_features']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Let's now use the processed tweet features to create a classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training-test Splitting\n",
    "\n",
    "To evaluate our approaches, we are going to split our data into train and validation. We will use the train to create the models and the validation to validate their performance. Once we have selected the best model (according to the accuracy on the validation set) we can use this model to predict our test set.\n",
    "\n",
    "In this way, test set will remain as unseen data for all the process: we are not going to make any decision based on the test error. Therefore, we can assume that the results on the test set will be the same that we will obtain when new unseen data appears in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "training_size = 4000\n",
    "train_tweets = [(tweet, sentiment) for tweet, sentiment in training_data[['tweet', 'sentiment']].values[:training_size]]\n",
    "validation_tweets  = [(tweet, sentiment) for tweet, sentiment in training_data[['tweet', 'sentiment']].values[training_size:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data for the classifier\n",
    "\n",
    "We have previously defined a feature extraction process, which we have wrapped into the `extract_features` function.\n",
    "\n",
    "By making use of the `nltk.classify.apply_features` function provided by NLTK, we will process the tweets and create the features that will be used for the classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Apply the data processing and cleaning extraction methodologies\n",
    "v_train = nltk.classify.apply_features(extract_features,train_tweets)\n",
    "v_validation  = nltk.classify.apply_features(extract_features,validation_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the resultant object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the tweet =  showing my mum twitter. \n",
      " \n",
      "The following features has been created:\n",
      " \n",
      "{u'neg_l(show)': 0.0, 'has(twitter)': 1, 'has(show,mum,twitter)': 1, u'has(show)': 1, u'neg_r(show)': 0.0, 'has(mum,twitter)': 1, 'neg_l(twitter)': 0.0, 'has(mum)': 1, 'neg_r(mum)': 0.0, 'has(show,mum)': 1, 'neg_r(twitter)': 0.0, 'neg_l(mum)': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"For the tweet = \", training_data.tweet.values[0])\n",
    "print(\" \")\n",
    "print(\"The following features has been created:\")\n",
    "print(\" \")\n",
    "print(v_train[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "We will start with a simple Naïve Bayes Classifier. For a given tweet, if we need to find the label for it, we find the probabilities of all the labels, given that feature and then select the label with maximum probability.\n",
    "\n",
    "NLTK has its own implementation of Naive Bayes `nltk.classify.NaiveBayesClassifier`. If you prefer, you can use the Naive Bayes implementation in `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "nb_classifier = nltk.classify.NaiveBayesClassifier\n",
    "nb_class = nb_classifier.train(v_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "Let's evaluate the accuracy of our model in our validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model =  0.711\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the model = \", nltk.classify.accuracy(nb_class, v_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "71.4 % of accuracy seems pretty good for the task.\n",
    "\n",
    "We can have a more detailed idea of the performance by taking a look to the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      " \n",
      "  |   0   4 |\n",
      "--+---------+\n",
      "0 |<367>146 |\n",
      "4 | 143<344>|\n",
      "--+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build confusion matrix over validation set\n",
    "test_truth   = [s for (t,s) in v_validation]\n",
    "test_predict = [nb_class.classify(t) for (t,s) in v_validation]\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print()\n",
    "print(nltk.ConfusionMatrix( test_truth, test_predict ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most Representative Features\n",
    "\n",
    "The NLTK classifier object allows us to see the most representative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "               has(poor) = 1                   0 : 4      =     16.4 : 1.0\n",
      "             neg_r(poor) = 0.0                 0 : 4      =     16.4 : 1.0\n",
      "             neg_l(poor) = 0.0                 0 : 4      =     15.1 : 1.0\n",
      "              neg_l(sad) = 0.0                 0 : 4      =     14.8 : 1.0\n",
      "               has(suck) = 1                   0 : 4      =     13.7 : 1.0\n",
      "                has(sad) = 1                   0 : 4      =     13.6 : 1.0\n",
      "             neg_l(suck) = 0.0                 0 : 4      =     12.9 : 1.0\n",
      "             neg_r(suck) = 0.0                 0 : 4      =     12.2 : 1.0\n",
      "              neg_r(sad) = 0.0                 0 : 4      =     11.9 : 1.0\n",
      "              neg_l(pic) = 0.0                 4 : 0      =     10.7 : 1.0\n",
      "             neg_l(glad) = 0.0                 4 : 0      =     10.6 : 1.0\n",
      "               has(glad) = 1                   4 : 0      =     10.6 : 1.0\n",
      "         has(good,night) = 1                   4 : 0      =     10.0 : 1.0\n",
      "               has(pain) = 1                   0 : 4      =     10.0 : 1.0\n",
      "             neg_l(sick) = 0.0                 0 : 4      =      9.5 : 1.0\n",
      "             neg_r(pain) = 0.0                 0 : 4      =      9.3 : 1.0\n",
      "             has(welcom) = 1                   4 : 0      =      9.3 : 1.0\n",
      "          has(good,luck) = 1                   4 : 0      =      9.3 : 1.0\n",
      "  has(__punc_excl,thank) = 1                   4 : 0      =      9.3 : 1.0\n",
      "           neg_r(welcom) = 0.0                 4 : 0      =      9.3 : 1.0\n",
      "             neg_r(sick) = 0.0                 0 : 4      =      9.2 : 1.0\n",
      "           neg_l(welcom) = 0.0                 4 : 0      =      8.9 : 1.0\n",
      "             neg_l(feel) = 0.9                 0 : 4      =      8.7 : 1.0\n",
      "             neg_l(pain) = 0.0                 0 : 4      =      8.7 : 1.0\n",
      "              neg_l(ugh) = 0.0                 0 : 4      =      8.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "nb_class.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "We have applied a thorough process to create features for our tweets. However, is it justified? Have we actually created a better representation of our data? To know that, we are going to create a baseline model that uses only the text in the tweets (with no features added).\n",
    "\n",
    "To that end we define a new extraction function that only extract the terms from the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "baseline_train_tweets = [(tweet.split(\" \"), sentiment) for tweet, sentiment in training_data[['tweet', 'sentiment']].values[:training_size]]\n",
    "baseline_validation_tweets  = [(tweet.split(\" \"), sentiment) for tweet, sentiment in training_data[['tweet', 'sentiment']].values[training_size:]]\n",
    "\n",
    "# Wrapper function for the extraction of features\n",
    "def extract_baseline_features(words):\n",
    "    \n",
    "    bag = {}\n",
    "    words_uni = [ 'has(%s)'% ug for ug in words ]\n",
    "    \n",
    "    for f in words_uni:\n",
    "        bag[f] = 1\n",
    "\n",
    "    return bag\n",
    "\n",
    "v_baseline_train = nltk.classify.apply_features(extract_baseline_features, baseline_train_tweets)\n",
    "v_baseline_validation = nltk.classify.apply_features(extract_baseline_features, baseline_validation_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit a new naive based classifier over this baseline representation and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "baseline_nb_classifier = nltk.classify.NaiveBayesClassifier\n",
    "baseline_nb_class = nb_classifier.train(v_baseline_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the baseline model =  0.669\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the baseline model = \", nltk.classify.accuracy(baseline_nb_class, v_baseline_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      " \n",
      "  |   0   4 |\n",
      "--+---------+\n",
      "0 |<389>124 |\n",
      "4 | 254<233>|\n",
      "--+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build confusion matrix over validation set\n",
    "test_truth   = [s for (t,s) in v_baseline_validation]\n",
    "test_predict = [nb_class.classify(t) for (t,s) in v_baseline_validation]\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print()\n",
    "print(nltk.ConfusionMatrix( test_truth, test_predict ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, performance is significantly lower than that of the model using all the features we have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             has(Thanks) = 1                   4 : 0      =     12.6 : 1.0\n",
      "                has(sad) = 1                   0 : 4      =     11.7 : 1.0\n",
      "               has(hate) = 1                   0 : 4      =     10.6 : 1.0\n",
      "               has(glad) = 1                   4 : 0      =     10.0 : 1.0\n",
      "              has(sucks) = 1                   0 : 4      =      8.0 : 1.0\n",
      "               has(Glad) = 1                   4 : 0      =      7.9 : 1.0\n",
      "              has(Happy) = 1                   4 : 0      =      6.8 : 1.0\n",
      "              has(Thank) = 1                   4 : 0      =      6.8 : 1.0\n",
      "               has(told) = 1                   0 : 4      =      6.8 : 1.0\n",
      "               has(sick) = 1                   0 : 4      =      6.8 : 1.0\n",
      "           has(morning!) = 1                   4 : 0      =      6.6 : 1.0\n",
      "                  has()) = 1                   4 : 0      =      6.6 : 1.0\n",
      "              has(tried) = 1                   0 : 4      =      6.1 : 1.0\n",
      "               has(Back) = 1                   0 : 4      =      6.1 : 1.0\n",
      "             has(throat) = 1                   0 : 4      =      6.1 : 1.0\n",
      "             has(lovely) = 1                   4 : 0      =      6.0 : 1.0\n",
      "                has(car) = 1                   0 : 4      =      6.0 : 1.0\n",
      "               has(free) = 1                   4 : 0      =      5.9 : 1.0\n",
      "               has(hour) = 1                   0 : 4      =      5.7 : 1.0\n",
      "               has(cold) = 1                   0 : 4      =      5.5 : 1.0\n",
      "                has(due) = 1                   0 : 4      =      5.5 : 1.0\n",
      "           has(headache) = 1                   0 : 4      =      5.5 : 1.0\n",
      "              has(bored) = 1                   0 : 4      =      5.2 : 1.0\n",
      "           has(birthday) = 1                   4 : 0      =      5.2 : 1.0\n",
      "        has(@mileycyrus) = 1                   4 : 0      =      5.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Most Representative Features\n",
    "baseline_nb_class.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: MaxEnt Classifier\n",
    "\n",
    "Let's try a more sophisticated classifier to see if we can boost the classification performance. In particular we will apply a Maximum Entropy Classifier. This classifier works by finding a probability distribution that maximizes the likelihood of testable data.\n",
    "\n",
    "To create a MaxEnt model, make use of the `nltk.classify.MaxentClassifier` function and follow the Naive Bayes example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentiWordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the theoretical session we presented some sentiment resources that could be used to enrich our dataset with external information.\n",
    "\n",
    "In particular, SentiWordNet provides a sentiment annotation for the WordNet synsets. We can add this sentiment annotation as new features to our dataset. \n",
    "\n",
    "In the following, we define a fuction that based on the words in the tweets and their POS tagging, find the sentiment annotation for the word_POS_TAG in SentiWordNet. We then add these values as new features in our dataset and use them to train a new MaxEnt Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "def penn_to_wn(tag):\n",
    "    \"\"\"\n",
    "    Convert between the PennTreebank tags to simple Wordnet tags\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    " \n",
    "\n",
    "def swn_polarity(text):\n",
    "    sentiment = 0.0\n",
    "    tokens_count = 0\n",
    "  \n",
    "    tagged_sentence = pos_tag(word_tokenize(unicode(text, errors='ignore')))\n",
    "    sentiment = {}\n",
    "    for word, tag in tagged_sentence:\n",
    "        \n",
    "        wn_tag = penn_to_wn(tag)\n",
    "        if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "            sentiment[\"sent(\"+word+\")\"] = 0.0\n",
    "            continue\n",
    "        \n",
    "        lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "        if not lemma:\n",
    "            sentiment[\"sent(\"+word+\")\"] = 0.0\n",
    "            continue\n",
    "\n",
    "        synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "        if not synsets:\n",
    "            sentiment[\"sent(\"+word+\")\"] = 0.0\n",
    "            continue\n",
    "\n",
    "        # Take the first sense, the most common\n",
    "        synset = synsets[0]\n",
    "        swn_synset = swn.senti_synset(synset.name())\n",
    "\n",
    "        sentiment[\"sent(\"+word+\")\"] = swn_synset.pos_score() - swn_synset.neg_score()\n",
    "        \n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sent(This)': 0.0,\n",
       " 'sent(a)': 0.0,\n",
       " 'sent(and)': 0.0,\n",
       " 'sent(bad)': -0.625,\n",
       " 'sent(good)': 0.75,\n",
       " 'sent(is)': 0.0,\n",
       " 'sent(stupid)': -0.75,\n",
       " 'sent(text)': 0.0,\n",
       " 'sent(very)': 0.0,\n",
       " 'sent(with)': 0.0,\n",
       " 'sent(words)': 0.0}"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is a text with good and very good words and bad and stupid words\"\n",
    "swn_polarity(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This annotation provides a sentiment score (based on the SentiWordNet sentiment score) for each term in the tweets (-1 negative, 1 positive, 0 neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Wrapper function for the extraction of features + sentiment features\n",
    "def extract_features_with_sentiment(text):\n",
    "    features = {}\n",
    "    \n",
    "    words = processAll(text)\n",
    "    \n",
    "    sentiment_features = swn_polarity(text)\n",
    "    features.update(sentiment_features)\n",
    "    \n",
    "    word_features = get_word_features(words)\n",
    "    features.update( word_features )\n",
    "\n",
    "    negation_features = get_negation_features(words)\n",
    "    features.update( negation_features )\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Enhanced classifier\n",
    "\n",
    "We are going to test if the sentiment lexicon improves our MaxEnt classifier. To that end you have to make use of the `extract_features_with_sentiment` function to create the features (by using the `nltk.classify.apply_features` function) to feed the classifier. **(take a look to the Naive Bayes example)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
