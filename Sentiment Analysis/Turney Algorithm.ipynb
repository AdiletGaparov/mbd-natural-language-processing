{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turney Algorithm\n",
    "\n",
    "In this notebook we will implement the Turney Algorithm (proposed by Peter D. Turney: http://www.aclweb.org/anthology/P02-1053.pdf) for automatically create a sentiment lexicon from our data. To that end, we will use a set of IMDB movie reviews classified into positive and negative.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Needed imports\n",
    "\n",
    "import os\n",
    "import math\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "We start by loading the positive and negative reviews from the data folder (there are 1000 reviews per class).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "data_path = \"./imdb1/\"\n",
    "\n",
    "path_pos = data_path + \"pos\"\n",
    "path_neg = data_path + \"neg\"\n",
    "\n",
    "pos_filenames = os.listdir(path_pos)\n",
    "neg_filenames = os.listdir(path_neg)\n",
    "\n",
    "contents=[]\n",
    "# Read the text in the positive files\n",
    "for f in pos_filenames:\n",
    "    with open (path_pos+ \"\\\\\" +f) as txt:\n",
    "        for line in txt:\n",
    "            contents.append(line)\n",
    "        \n",
    "# Read the text in the negativee files\n",
    "for f in neg_filenames:\n",
    "    with open (path_neg+ \"\\\\\" +f) as txt:\n",
    "        for line in txt:\n",
    "            contents.append(line)\n",
    "                  \n",
    "# Join the whole contents and split it by word\n",
    "res='\\n'.join(contents).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['films',\n",
       " 'adapted',\n",
       " 'from',\n",
       " 'comic',\n",
       " 'books',\n",
       " 'have',\n",
       " 'had',\n",
       " 'plenty',\n",
       " 'of',\n",
       " 'success']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in class, the first step was based on extracting two-word phrases with adjectives. To allow this, we have to first annotate the words with the POS tagging.\n",
    "\n",
    "We use the `pos_tag` function implemented into NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "pos_tags=pos_tag(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('films', 'NNS'),\n",
       " ('adapted', 'VBD'),\n",
       " ('from', 'IN'),\n",
       " ('comic', 'JJ'),\n",
       " ('books', 'NNS'),\n",
       " ('have', 'VBP'),\n",
       " ('had', 'VBD'),\n",
       " ('plenty', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('success', 'NN')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a function to find the patterns defined in the paper.\n",
    "\n",
    "    |First Word    |Second Word    |Third Word (not extracted)|\n",
    "    ***********************************************************\n",
    "    |JJ\t        |NN or NNS\t  |anything                  |\n",
    "    |RB, RBR, RBS  |JJ\t         |Not NN nor NNS            |\n",
    "    |JJ\t        |JJ\t         |Not NN or NNS             |\n",
    "    |NN or NNS\t |JJ\t         |Nor NN nor NNS            |\n",
    "    |RB,RBR or RBS |VB,VBD,VBN,VBG |anything                  |\n",
    "    ***********************************************************\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def find_pattern(postag):\n",
    "    tag_pattern = []  \n",
    "    for k in range(len(postag)-2):\n",
    "        if( postag[k][1]==\"JJ\" and postag[k+1][1]==\"JJ\" and postag[k+2][1]!=\"NN\" and postag[k+2][1]!=\"NNS\"):\n",
    "            tag_pattern.append(\"\".join(postag[k][0])+\" \"+\"\".join(postag[k+1][0]))\n",
    "        if( (postag[k][1]==\"NN\" or postag[k][1]==\"NNS\") and postag[k+1][1]==\"JJ\" and postag[k+2][1]!=\"NN\" and postag[k+2][1]!=\"NNS\"):\n",
    "            tag_pattern.append(\"\".join(postag[k][0])+\" \"+\"\".join(postag[k+1][0]))\n",
    "        if( (postag[k][1]==\"RB\" or postag[k][1]==\"RBR\" or postag[k][1]==\"RBS\") and postag[k+1][1]==\"JJ\" and postag[k+2][1]!=\"NN\" and postag[k+2][1]!=\"NNS\"):\n",
    "            tag_pattern.append(\"\".join(postag[k][0])+\" \"+\"\".join(postag[k+1][0]))\n",
    "        if( (postag[k][1]==\"RB\" or postag[k][1]==\"RBR\" or postag[k][1]==\"RBS\") and (postag[k+1][1]==\"VB\" or postag[k+1][1]==\"VBN\" or postag[k+1][1]==\"VBD\" or postag[k+1][1]==\"VBG\")):\n",
    "            tag_pattern.append(\"\".join(postag[k][0])+\" \"+\"\".join(postag[k+1][0])) \n",
    "        if( postag[k][1]==\"JJ\" and postag[k+1][1]==\"NN\" ) or ( postag[k][1]==\"JJ\" and postag[k+1][1]==\"NNS\" ):\n",
    "            tag_pattern.append(\"\".join(postag[k][0])+\" \"+\"\".join(postag[k+1][0]))\n",
    "    return tag_pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function to store all phrases that satisfy the conditions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['not developing',\n",
       " '\" watch',\n",
       " 'andy dick',\n",
       " 'definite difference',\n",
       " 'bio-mechanical life-forms',\n",
       " 'sharp term',\n",
       " 'gigantic neighbor',\n",
       " 'altar due',\n",
       " 'stupid screenplay',\n",
       " 'uninspiring man']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_pattern = find_pattern(pos_tags)\n",
    "tag_pattern = list(set(tag_pattern))\n",
    "tag_pattern[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create three data structures to facilitate the algorithm execution\n",
    "\n",
    "- **mat_phrase_great:** numpy matrix of hits between each phrase and the word great\n",
    "- **mat_phrase_poor:** hits between phrase and poor in each file\n",
    "- **mat_phrase_count:** matrix storing 1 if a phrase is present in a file. used for adding corresponding SOs later.\n",
    "- **hits_great:** stores total hits of great in training set for each fold, correspondingly **hits_poor** stores poor hits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "mat_phrase_great= np.zeros((len(tag_pattern), len(pos_filenames) + len(neg_filenames)), dtype=\"int8\")\n",
    "mat_phrase_poor= np.zeros((len(tag_pattern),  len(pos_filenames) + len(neg_filenames)), dtype=\"int8\")\n",
    "mat_phrase_count=np.zeros((len(tag_pattern), len(pos_filenames) + len(neg_filenames)), dtype=\"int8\")\n",
    "hits_great=[]\n",
    "hits_poor=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following counts the ocurrence of the words `poor` and `great` in the positive files.\n",
    "\n",
    "Be patient! it is going to take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "for cnt, fi in enumerate(pos_filenames):\n",
    "    with open (path_pos + \"\\\\\" +fi) as cf:\n",
    "        txt=cf.read()\n",
    "        txt = \"\".join(l for l in txt if l not in string.punctuation)\n",
    "        file_list=txt.split()\n",
    "        hits_great.append(file_list.count(\"great\"))\n",
    "        hits_poor.append(file_list.count(\"poor\"))\n",
    "\n",
    "        for j in range(len(tag_pattern)):\n",
    "            all_hit_phrase_index=[]\n",
    "            hits_phrase_great=0\n",
    "            hits_phrase_poor=0\n",
    "            if (tag_pattern[j] in txt):\n",
    "                mat_phrase_count[j][cnt]=1\n",
    "                try:\n",
    "                    for w in (file_list):\n",
    "                        if (w==tag_pattern[j].split()[0]):\n",
    "                            ind=file_list.index(w)\n",
    "                            if(file_list[ind+1]==tag_pattern[j].split()[1]):\n",
    "                                #print(ind)\n",
    "                                all_hit_phrase_index.append(ind)\n",
    "                        for ids in (all_hit_phrase_index):\n",
    "                            #print(all_hit_index)\n",
    "                            for words in file_list[ids-10 :ids+11]:\n",
    "                                if words==\"great\":\n",
    "                                    hits_phrase_great+=1\n",
    "                                if words==\"poor\":\n",
    "                                    hits_phrase_poor+=1\n",
    "                        mat_phrase_great[j][cnt]=hits_phrase_great\n",
    "                        mat_phrase_poor[j][cnt]=hits_phrase_poor\n",
    "                except:\n",
    "                        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same for the negative files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for cnt, fi in enumerate(neg_filenames):    \n",
    "    with open (path_neg + \"\\\\\" +fi) as cf:\n",
    "            txt=cf.read()\n",
    "            file_list=txt.split()\n",
    "            hits_great.append(file_list.count(\"great\"))\n",
    "            hits_poor.append(file_list.count(\"poor\"))\n",
    "            for j in range(len(tag_pattern)):\n",
    "                all_hit_phrase_index=[]\n",
    "                hits_phrase_great=0\n",
    "                hits_phrase_poor=0\n",
    "                if (tag_pattern[j] in txt):\n",
    "                    mat_phrase_count[j][cnt]=1\n",
    "                    try:\n",
    "                        for w in (file_list):\n",
    "                            if (w==tag_pattern[j].split()[0]):\n",
    "                                ind=file_list.index(w)\n",
    "                                if(file_list[ind+1]==tag_pattern[j].split()[1]):\n",
    "                                    #print(ind)\n",
    "                                    all_hit_phrase_index.append(ind)\n",
    "                        for ids in (all_hit_phrase_index):\n",
    "                            #print(all_hit_phrase_index)\n",
    "                            for words in file_list[ids-10 :ids+11]:\n",
    "                                if words==\"great\":\n",
    "                                    hits_phrase_great+=1\n",
    "                                if words==\"poor\":\n",
    "                                    hits_phrase_poor+=1                     \n",
    "                        mat_phrase_great[j][cnt]=hits_phrase_great\n",
    "                        mat_phrase_poor[j][cnt]=hits_phrase_poor\n",
    "                    except:\n",
    "                            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this count matrices, we can now calculate the semantic orientation of test data.\n",
    "\n",
    "The following sentence takes the test_data to annoate and the counts to predict the orientation of each review in the test data (by using the Pointwise mutual information). It also evaluates the prediction compared to the actual label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_all=[]\n",
    "def semantic_orientation(p_hit_great, p_hit_poor, hits_gr, hits_po, test_data):\n",
    "    num=(p_hit_great*float(hits_po))+0.01\n",
    "    den=(p_hit_poor*float(hits_gr))+0.01\n",
    "    so=np.log2(np.divide(num, den)) #PMI\n",
    "    so=np.nan_to_num(so) # Change nan to 0 to avoid errors\n",
    "    acc=0.0\n",
    "    fold_no=0\n",
    "    for f in test_data:\n",
    "        polarity=0.0\n",
    "        \n",
    "        if f<1000:\n",
    "            correct_label=\"positive\"\n",
    "            \n",
    "        if f>=1000:\n",
    "            correct_label=\"negative\"\n",
    "        \n",
    "        for p in range(len(so)):\n",
    "            if mat_phrase_count[p][f]==1:\n",
    "                polarity+=so[p]\n",
    "                \n",
    "        if (polarity>=0.1): # Confidence threshold\n",
    "            pred=\"positive\"\n",
    "            \n",
    "        else:\n",
    "            pred=\"negative\"\n",
    "            \n",
    "        if(pred==correct_label):\n",
    "             acc+=1\n",
    "             \n",
    "    acc=acc/float(200)\n",
    "    acc_all.append(acc)\n",
    "    \n",
    "    print(\"[INFO] Fold accuracy: %r\" %(acc))               \n",
    "    fold_no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we apply the latter function to the data.\n",
    "\n",
    "We use a 10-fold Cross-Validation to predict and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angel\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in log2\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Fold accuracy: 0.975\n",
      "[INFO] Fold accuracy: 0.98\n",
      "[INFO] Fold accuracy: 0.995\n",
      "[INFO] Fold accuracy: 0.97\n",
      "[INFO] Fold accuracy: 0.98\n",
      "[INFO] Fold accuracy: 0.99\n",
      "[INFO] Fold accuracy: 0.97\n",
      "[INFO] Fold accuracy: 0.98\n",
      "[INFO] Fold accuracy: 0.97\n",
      "[INFO] Fold accuracy: 0.985\n",
      "[INFO] Accuracy: 0.9795 \n"
     ]
    }
   ],
   "source": [
    "\"\"\"10 Fold Cross-Validation\"\"\"\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf=KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "# Join the negative and positive filenames\n",
    "file_names=os.listdir(path_pos)+os.listdir(path_neg)\n",
    "\n",
    "for train, test_data in kf.split(file_names):  ##for each fold in the 10 fold CV\n",
    "    tr_great= mat_phrase_great[: , train[0]:train[-1]]\n",
    "    phrase_hit_great=np.sum(tr_great, axis=1)\n",
    "    tr_poor= mat_phrase_poor[: , train[0]:train[-1]]\n",
    "    phrase_hit_poor=np.sum(tr_poor, axis=1)\n",
    "   \n",
    "    hits_gr=sum(hits_great[train[0]:train[-1]])\n",
    "    hits_po=sum(hits_poor[train[0]:train[-1]])\n",
    "   \n",
    "    semantic_orientation(phrase_hit_great, phrase_hit_poor, hits_gr, hits_po, test_data)   \n",
    "    \n",
    "acc_avg=sum(acc_all)/float(10)\n",
    "print(\"[INFO] Accuracy: %r \" %(acc_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Almost perfect classification!**\n",
    "\n",
    "We have proven that even from a small dataset (2000 reviews) we can create a very good sentiment lexicon.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
