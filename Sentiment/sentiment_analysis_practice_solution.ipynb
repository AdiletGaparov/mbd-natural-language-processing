{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Sentiment Analysis refers to the use of text analysis and natural language processing to identify and extract subjective information in textual contents.\n",
    "\n",
    "In this practice we will focus on the analysis of the sentiment of a collection of tweets, applying some of the ideas that we have explored in class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "This corpus of tweets, developed by Sanfordâ€™s Natural Language processing research group.\n",
    "\n",
    "The training set is collected by querying Twitter API for happy emoticons like \":)\" and sad emoticons like \":(\" and labelling them positive or negative. The emoticons were then stripped and Re-Tweets and duplicates removed.\n",
    "\n",
    "The data is a CSV with emoticons removed. Data file format has 6 fields:\n",
    "\n",
    "    0 - the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive) **Note**: For the dataset there is only negative and positive tweets\n",
    "    1 - the id of the tweet (2087)\n",
    "    2 - the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "    3 - the query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "    4 - the user that tweeted (robotickilldozr)\n",
    "    5 - the text of the tweet (Lyx is cool)\n",
    "\n",
    "It also contains around 500 tweets manually collected and labelled for testing purposes.\n",
    "\n",
    "We randomly sample and use 5000 tweets from this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import csv\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "def loadDataset(in_file):\n",
    "    my_path = os.getcwd()\n",
    "    path = os.path.join(my_path, in_file)\n",
    "    column_names = ['sentiment','ID', 'Date', 'Query', 'user_id', 'tweet']\n",
    "    tweets = pd.read_csv(path, delimiter=',', quotechar='\"', header= None, names= column_names, encoding=\"ISO-8859-1\")\n",
    "\n",
    "    print('Readed ', len(tweets), \"tweets\")\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data and check the number of positive and negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readed  1600000 tweets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "0    800000\n",
       "4    800000\n",
       "Name: ID, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_training_data = loadDataset(\"stanford_dataset/training.1600000.processed.noemoticon.csv\")\n",
    "raw_training_data.groupby('sentiment')['ID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset contains more than a million tweets, for our practice we will only use a sample of 5000 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Query</th>\n",
       "      <th>user_id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>989354</th>\n",
       "      <td>4</td>\n",
       "      <td>1834864714</td>\n",
       "      <td>Mon May 18 04:26:58 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joanakitty</td>\n",
       "      <td>Trying to find my DIARY when i was in elementary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310512</th>\n",
       "      <td>4</td>\n",
       "      <td>2013329739</td>\n",
       "      <td>Tue Jun 02 22:25:10 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>lkthigpen</td>\n",
       "      <td>The sole purpose of this update is to annoy @s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356794</th>\n",
       "      <td>0</td>\n",
       "      <td>2044400340</td>\n",
       "      <td>Fri Jun 05 09:18:11 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>IcemanYVR</td>\n",
       "      <td>@michael_luu Nice to see GM is using our tax d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196740</th>\n",
       "      <td>4</td>\n",
       "      <td>1984848950</td>\n",
       "      <td>Sun May 31 15:55:54 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>esoterismo</td>\n",
       "      <td>@wFranke @iamwun @stayingyoung @MsFitUniverse ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622987</th>\n",
       "      <td>0</td>\n",
       "      <td>2229400550</td>\n",
       "      <td>Thu Jun 18 15:40:44 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>DeeRose</td>\n",
       "      <td>@LMRB no sun up north girl  oh btw ill be in m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment          ID                          Date     Query  \\\n",
       "989354           4  1834864714  Mon May 18 04:26:58 PDT 2009  NO_QUERY   \n",
       "1310512          4  2013329739  Tue Jun 02 22:25:10 PDT 2009  NO_QUERY   \n",
       "356794           0  2044400340  Fri Jun 05 09:18:11 PDT 2009  NO_QUERY   \n",
       "1196740          4  1984848950  Sun May 31 15:55:54 PDT 2009  NO_QUERY   \n",
       "622987           0  2229400550  Thu Jun 18 15:40:44 PDT 2009  NO_QUERY   \n",
       "\n",
       "            user_id                                              tweet  \n",
       "989354   joanakitty  Trying to find my DIARY when i was in elementary   \n",
       "1310512   lkthigpen  The sole purpose of this update is to annoy @s...  \n",
       "356794    IcemanYVR  @michael_luu Nice to see GM is using our tax d...  \n",
       "1196740  esoterismo  @wFranke @iamwun @stayingyoung @MsFitUniverse ...  \n",
       "622987      DeeRose  @LMRB no sun up north girl  oh btw ill be in m...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample 5000 tweets from the dataset\n",
    "training_data = raw_training_data.sample(n=5000)\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the distribution of positive and negative tweets remains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "0    2549\n",
       "4    2451\n",
       "Name: ID, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.groupby('sentiment')['ID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate the interpretation of the results we are going to recode the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Query</th>\n",
       "      <th>user_id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>989354</th>\n",
       "      <td>positive</td>\n",
       "      <td>1834864714</td>\n",
       "      <td>Mon May 18 04:26:58 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joanakitty</td>\n",
       "      <td>Trying to find my DIARY when i was in elementary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310512</th>\n",
       "      <td>positive</td>\n",
       "      <td>2013329739</td>\n",
       "      <td>Tue Jun 02 22:25:10 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>lkthigpen</td>\n",
       "      <td>The sole purpose of this update is to annoy @s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356794</th>\n",
       "      <td>negative</td>\n",
       "      <td>2044400340</td>\n",
       "      <td>Fri Jun 05 09:18:11 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>IcemanYVR</td>\n",
       "      <td>@michael_luu Nice to see GM is using our tax d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196740</th>\n",
       "      <td>positive</td>\n",
       "      <td>1984848950</td>\n",
       "      <td>Sun May 31 15:55:54 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>esoterismo</td>\n",
       "      <td>@wFranke @iamwun @stayingyoung @MsFitUniverse ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622987</th>\n",
       "      <td>negative</td>\n",
       "      <td>2229400550</td>\n",
       "      <td>Thu Jun 18 15:40:44 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>DeeRose</td>\n",
       "      <td>@LMRB no sun up north girl  oh btw ill be in m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment          ID                          Date     Query  \\\n",
       "989354   positive  1834864714  Mon May 18 04:26:58 PDT 2009  NO_QUERY   \n",
       "1310512  positive  2013329739  Tue Jun 02 22:25:10 PDT 2009  NO_QUERY   \n",
       "356794   negative  2044400340  Fri Jun 05 09:18:11 PDT 2009  NO_QUERY   \n",
       "1196740  positive  1984848950  Sun May 31 15:55:54 PDT 2009  NO_QUERY   \n",
       "622987   negative  2229400550  Thu Jun 18 15:40:44 PDT 2009  NO_QUERY   \n",
       "\n",
       "            user_id                                              tweet  \n",
       "989354   joanakitty  Trying to find my DIARY when i was in elementary   \n",
       "1310512   lkthigpen  The sole purpose of this update is to annoy @s...  \n",
       "356794    IcemanYVR  @michael_luu Nice to see GM is using our tax d...  \n",
       "1196740  esoterismo  @wFranke @iamwun @stayingyoung @MsFitUniverse ...  \n",
       "622987      DeeRose  @LMRB no sun up north girl  oh btw ill be in m...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def recode_sentiment(series):\n",
    "    if series == 4:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'\n",
    "    \n",
    "training_data['sentiment'] = training_data['sentiment'].apply(recode_sentiment)\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Preprocessing\n",
    "\n",
    "At this step, we will preprocess the text in the tweets, tokenize and stem it. We will have to take care of specific markups (e.g., hashtags) related to Twitter, as well as of aspects related to the sentiment analysis, like, for instance, emoticons.\n",
    "\n",
    "You may find interesting ideas in this regard in the following links:\n",
    " - Christopher Potts sentiment tokenizer: http://sentiment.christopherpotts.net/code-data/happyfuntokenizing.py\n",
    " - Brendan Oâ€™Connor twitter tokenizer: https://github.com/brendano/tweetmotif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashtags\n",
    "\n",
    "A hashtag is a word or an un-spaced phrase prefixed with the hash symbol (#). These are used to both naming subjects and phrases that are currently in trending topics. For example, #iPad, #news\n",
    "\n",
    "    Regular Expression: #(\\w+)\n",
    "\n",
    "    Replace Expression: HASH_hashtag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "hash_regex = re.compile(r\"#(\\w+)\")\n",
    "def hash_repl(match):\n",
    "\treturn '__HASH_'+match.group(1).upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happy midsummer everyone! My little brother has a bd today and here are few relatives having a dinner.. not so sure is it very nice and __HASH_HASHTAG'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "re.sub( hash_regex, hash_repl, 'happy midsummer everyone! My little brother has a bd today and here are few relatives having a dinner.. not so sure is it very nice and #hashtag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Names\n",
    "Every Twitter user has a unique username. Any thing directed towards that user can be indicated be writing their username preceded by â€˜@â€™. Thus, these are like proper nouns. For example, @Apple\n",
    "\n",
    "    Regular Expression: @(\\w+)\n",
    "\n",
    "    Replace Expression: user_username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_regex = re.compile(r\"@(\\w+)\")\n",
    "def user_repl(match):\n",
    "\treturn '__user_'+match.group(1).upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a __user_USERNAME'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "re.sub( user_regex, user_repl, 'This is a @username')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URLs\n",
    "Users often share hyperlinks in their tweets. Twitter shortens them using its in-house URL shortening service, like http://t.co/FCWXoUd8 - such links also enables Twitter to alert users if the link leads out of its domain. From the point of view of text classification, a particular URL is not important. However, presence of a URL can be an important feature. Regular expression for detecting a URL is fairly complex because of different types of URLs that can be there, but because of Twitterâ€™s shortening service, we can use a relatively simple regular expression.\n",
    "\n",
    "    Regular Expression: (http|https|ftp)://[a-zA-Z0-9\\\\./]+\n",
    "\n",
    "    Replace Expression: URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_regex = re.compile(r\"(http|https|ftp)://[a-zA-Z0-9\\./]+\")\n",
    "def url_repl(match):\n",
    "\treturn '__URL_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a __URL_'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "re.sub( url_regex, url_repl, 'This is a http://url.es')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emoticons\n",
    "\n",
    "Use of emoticons is very prevalent throughout the web, more so on micro-blogging sites. We identify the following emoticons and replace them with a single word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emoticons\n",
    "emoticons = \\\n",
    "\t[\t('__EMOT_SMILEY',\t[':-)', ':)', '(:', '(-:', ] )\t,\\\n",
    "\t\t('__EMOT_LAUGH',\t\t[':-D', ':D', 'X-D', 'XD', 'xD', ] )\t,\\\n",
    "\t\t('__EMOT_LOVE',\t\t['<3', ':\\*', ] )\t,\\\n",
    "\t\t('__EMOT_WINK',\t\t[';-)', ';)', ';-D', ';D', '(;', '(-;', ] )\t,\\\n",
    "\t\t('__EMOT_FROWN',\t\t[':-(', ':(', '(:', '(-:', ] )\t,\\\n",
    "\t\t('__EMOT_CRY',\t\t[':,(', ':\\'(', ':\"(', ':(('] )\t,\\\n",
    "\t]\n",
    "    \n",
    "def escape_paren(arr):\n",
    "\treturn [text.replace(')', '[)}\\]]').replace('(', '[({\\[]') for text in arr]\n",
    "\n",
    "def regex_union(arr):\n",
    "\treturn '(' + '|'.join( arr ) + ')'\n",
    "\n",
    "emoticons_regex = [ (repl, re.compile(regex_union(escape_paren(regx))) ) for (repl, regx) in emoticons ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a text with one emoticon  __EMOT_SMILEY  and another  __EMOT_FROWN \n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "text = \"This is a text with one emoticon :) and another :(\"\n",
    "for (repl, regx) in emoticons_regex :\n",
    "    text = re.sub(regx, ' '+repl+' ', text)\n",
    "    \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuations\n",
    "\n",
    "Although not all Punctuations are important from the point of view of classification but some of these, like question mark, exclamation mark can also provide information about the sentiments of the text. We replace every word boundary by a list of relevant punctuations present at that point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting by word boundaries\n",
    "word_bound_regex = re.compile(r\"\\W+\")\n",
    "\n",
    "# Punctuations\n",
    "punctuations = \\\n",
    "\t[\t#('',\t\t['.', ] )\t,\\\n",
    "\t\t#('',\t\t[',', ] )\t,\\\n",
    "\t\t#('',\t\t['\\'', '\\\"', ] )\t,\\\n",
    "\t\t('__PUNC_EXCL',\t\t['!', 'Â¡', ] )\t,\\\n",
    "\t\t('__PUNC_QUES',\t\t['?', 'Â¿', ] )\t,\\\n",
    "\t\t('__PUNC_ELLP',\t\t['...', 'â€¦', ] )\t,\\\n",
    "\t]\n",
    "\n",
    "#For punctuation replacement\n",
    "def punctuations_repl(match):\n",
    "\ttext = match.group(0)\n",
    "\trepl = []\n",
    "\tfor (key, parr) in punctuations :\n",
    "\t\tfor punc in parr :\n",
    "\t\t\tif punc in text:\n",
    "\t\t\t\trepl.append(key)\n",
    "\tif( len(repl)>0 ) :\n",
    "\t\treturn ' '+' '.join(repl)+' '\n",
    "\telse :\n",
    "\t\treturn ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This a text with an exclamation __PUNC_EXCL '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "re.sub( word_bound_regex , punctuations_repl, \"This a text with an exclamation!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repetitions\n",
    "People often use repeating characters while using colloquial language, like \"Iâ€™m in a hurrryyyyy\", \"We won, yaaayyyyy!\" As our final pre-processing step, we replace characters repeating more than twice as two characters.\n",
    "\n",
    "    Regular Expression: (.)\\1{1,}\n",
    "\n",
    "    Replace Expression: \\1\\1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeating words like hurrrryyyyyy\n",
    "rpt_regex = re.compile(r\"(.)\\1{1,}\", re.IGNORECASE);\n",
    "def rpt_repl(match):\n",
    "\treturn match.group(1)+match.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reppeated characters in wordss'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "re.sub( rpt_regex, rpt_repl, \"Reppppeated characters in wordsssssssss\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "We will now stemmize the words in the tweets by applying the Porter Stemmer seen in class. This stemmer was very widely used and became and remains the de facto standard algorithm used for English stemming. It offers excellent trade-off between speed, readability, and accuracy.\n",
    "\n",
    "NLTK has its own implementation of the stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'textual represent contain word appli the porter stemmer'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "text = \"Textual representation containing words to apply the porter stemmer\"\n",
    "text = [word if(word[0:2]=='__') else word.lower() for word in text.split() if len(word) >= 3]\n",
    "text = [stemmer.stem(w) for w in text]                \n",
    "text = \" \".join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function that encloses all the processing procedures\n",
    "def processAll(text):\n",
    "    \n",
    "    text = re.sub( hash_regex, hash_repl, text )\n",
    "    text = re.sub( user_regex, user_repl, text)\n",
    "    text = re.sub( url_regex, ' __URL ', text )\n",
    "    \n",
    "    for (repl, regx) in emoticons_regex :\n",
    "        text = re.sub(regx, ' '+repl+' ', text)\n",
    "    \n",
    "    text = text.replace('\\'','')\n",
    "    \n",
    "    text = re.sub( word_bound_regex , punctuations_repl, text )\n",
    "    text = re.sub( rpt_regex, rpt_repl, text )\n",
    "    \n",
    "        \n",
    "    text = [word if(word[0:2]=='__') else word.lower() for word in text.split() if len(word) >= 3]\n",
    "    text = [stemmer.stem(w) for w in text]                \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new column in our dataframe with the processed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Query</th>\n",
       "      <th>user_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>processed_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>989354</th>\n",
       "      <td>positive</td>\n",
       "      <td>1834864714</td>\n",
       "      <td>Mon May 18 04:26:58 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joanakitty</td>\n",
       "      <td>Trying to find my DIARY when i was in elementary</td>\n",
       "      <td>[tri, find, diari, when, wa, elementari]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310512</th>\n",
       "      <td>positive</td>\n",
       "      <td>2013329739</td>\n",
       "      <td>Tue Jun 02 22:25:10 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>lkthigpen</td>\n",
       "      <td>The sole purpose of this update is to annoy @s...</td>\n",
       "      <td>[the, sole, purpos, thi, updat, annoy, __user_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356794</th>\n",
       "      <td>negative</td>\n",
       "      <td>2044400340</td>\n",
       "      <td>Fri Jun 05 09:18:11 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>IcemanYVR</td>\n",
       "      <td>@michael_luu Nice to see GM is using our tax d...</td>\n",
       "      <td>[__user_michael_luu, nice, see, use, our, tax,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196740</th>\n",
       "      <td>positive</td>\n",
       "      <td>1984848950</td>\n",
       "      <td>Sun May 31 15:55:54 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>esoterismo</td>\n",
       "      <td>@wFranke @iamwun @stayingyoung @MsFitUniverse ...</td>\n",
       "      <td>[__user_wfrank, __user_iamwun, __user_stayingy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622987</th>\n",
       "      <td>negative</td>\n",
       "      <td>2229400550</td>\n",
       "      <td>Thu Jun 18 15:40:44 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>DeeRose</td>\n",
       "      <td>@LMRB no sun up north girl  oh btw ill be in m...</td>\n",
       "      <td>[__user_lmrb, sun, north, girl, btw, ill, midd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment          ID                          Date     Query  \\\n",
       "989354   positive  1834864714  Mon May 18 04:26:58 PDT 2009  NO_QUERY   \n",
       "1310512  positive  2013329739  Tue Jun 02 22:25:10 PDT 2009  NO_QUERY   \n",
       "356794   negative  2044400340  Fri Jun 05 09:18:11 PDT 2009  NO_QUERY   \n",
       "1196740  positive  1984848950  Sun May 31 15:55:54 PDT 2009  NO_QUERY   \n",
       "622987   negative  2229400550  Thu Jun 18 15:40:44 PDT 2009  NO_QUERY   \n",
       "\n",
       "            user_id                                              tweet  \\\n",
       "989354   joanakitty  Trying to find my DIARY when i was in elementary    \n",
       "1310512   lkthigpen  The sole purpose of this update is to annoy @s...   \n",
       "356794    IcemanYVR  @michael_luu Nice to see GM is using our tax d...   \n",
       "1196740  esoterismo  @wFranke @iamwun @stayingyoung @MsFitUniverse ...   \n",
       "622987      DeeRose  @LMRB no sun up north girl  oh btw ill be in m...   \n",
       "\n",
       "                                           processed_tweet  \n",
       "989354            [tri, find, diari, when, wa, elementari]  \n",
       "1310512  [the, sole, purpos, thi, updat, annoy, __user_...  \n",
       "356794   [__user_michael_luu, nice, see, use, our, tax,...  \n",
       "1196740  [__user_wfrank, __user_iamwun, __user_stayingy...  \n",
       "622987   [__user_lmrb, sun, north, girl, btw, ill, midd...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['processed_tweet'] = training_data.tweet.apply(processAll)\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Creation\n",
    "\n",
    "A wide variety of features can be used to build a classifier for tweets. The most widely used and basic feature set is word n-grams. However, there's a lot of domain specific information present in tweets that can also be used for classifying them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigrams\n",
    "\n",
    "Unigrams are the simplest features that can be used for text classification. A Tweet can be represented by a multiset of words present in it. We, however, have used the presence of unigrams in a tweet as a feature set. Presence of a word is more important than how many times it is repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'Example': 1, 'of': 1, 'tweet': 1, 'represented': 1, 'as': 1, 'unigrams': 1})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"Example\", \"of\", \"tweet\", \"represented\", \"as\", \"unigrams\"]\n",
    "\n",
    "unigrams_fd = nltk.FreqDist()\n",
    "unigrams_fd.update(text)\n",
    "unigrams_fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams\n",
    "\n",
    "N-gram refers to an n-long sequence of words. Probabilistic Language Models based on Unigrams, Bigrams and Trigrams can be successfully used to predict the next word given a current context of words. In the domain of sentiment analysis, the performance of N-grams is unclear.\n",
    "\n",
    "As the order of the n-grams increases, they tend to be more and more sparse. Let's then try bi-gram and tri-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'Example,of': 1, 'of,tweet': 1, 'tweet,represented': 1, 'represented,as': 1, 'as,unigrams': 1})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bigrams\n",
    "words_bi  = [ ','.join(map(str,bg)) for bg in nltk.bigrams(text) ]\n",
    "bi_grams_fd = nltk.FreqDist()\n",
    "bi_grams_fd.update( words_bi )\n",
    "bi_grams_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'Example,of,tweet': 1, 'of,tweet,represented': 1, 'tweet,represented,as': 1, 'represented,as,unigrams': 1})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trigrams\n",
    "words_tri  = [ ','.join(map(str,tg)) for tg in nltk.trigrams(text) ]\n",
    "tri_grams_fd = nltk.FreqDist()\n",
    "tri_grams_fd.update( words_tri )\n",
    "tri_grams_fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the bigrams and trigrams models for the processed text in the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function that encloses all the n-grams procedures\n",
    "\n",
    "def get_word_features(words):\n",
    "    bag = {}\n",
    "    words_uni = [ 'has(%s)'% ug for ug in words ]\n",
    "    words_bi  = [ 'has(%s)'% ','.join(map(str,bg)) for bg in nltk.bigrams(words) ]\n",
    "    words_tri = [ 'has(%s)'% ','.join(map(str,tg)) for tg in nltk.trigrams(words) ]\n",
    "    \n",
    "    for f in words_uni+words_bi+words_tri:\n",
    "        bag[f] = 1\n",
    "\n",
    "    return bag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negations\n",
    "\n",
    "The need negation detection in sentiment analysis can be illustrated by the difference in the meaning of the phrases, \"This is good\" vs. \"This is not good\" However, the negations occurring in natural language are seldom so simple. Handling the negation consists of two tasks â€“ Detection of explicit negation cues and the scope of negation of these words.\n",
    "\n",
    "**Scope of Negation**\n",
    "\n",
    "Words immediately preceding and following the negation cues are the most negative and the words that come farther away do not lie in the scope of negation of such cues. We define left and right negativity of a word as the chances that meaning of that word is actually the opposite. Left negativity depends on the closest negation cue on the left and similarly for Right negativity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "negtn_regex = re.compile( r\"\"\"(?:\n",
    "    ^(?:never|no|nothing|nowhere|noone|none|not|\n",
    "        havent|hasnt|hadnt|cant|couldnt|shouldnt|\n",
    "        wont|wouldnt|dont|doesnt|didnt|isnt|arent|aint\n",
    "    )$\n",
    ")\n",
    "|\n",
    "n't\n",
    "\"\"\", re.X)\n",
    "\n",
    "def get_negation_features(words):\n",
    "    INF = 0.0\n",
    "    negtn = [ bool(negtn_regex.search(w)) for w in words ]\n",
    "\n",
    "    left = [0.0] * len(words)\n",
    "    prev = 0.0\n",
    "    for i in range(0,len(words)):\n",
    "        if( negtn[i] ):\n",
    "            prev = 1.0\n",
    "        left[i] = prev\n",
    "        prev = max( 0.0, prev-0.1)\n",
    "\n",
    "    right = [0.0] * len(words)\n",
    "    prev = 0.0\n",
    "    for i in reversed(range(0,len(words))):\n",
    "        if( negtn[i] ):\n",
    "            prev = 1.0\n",
    "        right[i] = prev\n",
    "        prev = max( 0.0, prev-0.1)\n",
    "\n",
    "    return dict( zip(\n",
    "                    ['neg_l('+w+')' for w in  words] + ['neg_r('+w+')' for w in  words],\n",
    "                    left + right ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg_l(This)': 0.0,\n",
       " 'neg_l(text)': 0.0,\n",
       " 'neg_l(does)': 0.0,\n",
       " 'neg_l(not)': 1.0,\n",
       " 'neg_l(have)': 0.9,\n",
       " 'neg_l(a)': 0.8,\n",
       " 'neg_l(negation)': 0.7000000000000001,\n",
       " 'neg_r(This)': 0.7000000000000001,\n",
       " 'neg_r(text)': 0.8,\n",
       " 'neg_r(does)': 0.9,\n",
       " 'neg_r(not)': 1.0,\n",
       " 'neg_r(have)': 0.0,\n",
       " 'neg_r(a)': 0.0,\n",
       " 'neg_r(negation)': 0.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "text = [\"This\",\"text\", \"does\", \"not\", \"have\", \"a\", \"negation\"]\n",
    "get_negation_features(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging\n",
    "\n",
    "With POS Tagging we can get the category of each word. Some of these categories are more interesting in order to infer the sentiment of given tweet. For example, adjectives are expected to carry most sentiment information than adverbs. In a similar way, some particular names can carry a positive or negative implication for particular domains.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_features(words):\n",
    "    tags = {}\n",
    "    tagged_words = [ 'has(%s)'% w+'_'+tag for w,tag in nltk.pos_tag(words)]\n",
    "    \n",
    "    for tw in tagged_words:\n",
    "        tags[tw] = 1\n",
    "\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous step, let's create an function to apply all these creation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for the extraction of features\n",
    "def extract_features(text):\n",
    "    features = {}\n",
    "    \n",
    "    words = processAll(text)\n",
    "\n",
    "    word_features = get_word_features(words)\n",
    "    features.update( word_features )\n",
    "\n",
    "    negation_features = get_negation_features(words)\n",
    "    features.update( negation_features )\n",
    "    \n",
    "#     pos_features = get_pos_features(words)\n",
    "#     features.update( pos_features )\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>processed_tweet_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>989354</th>\n",
       "      <td>Trying to find my DIARY when i was in elementary</td>\n",
       "      <td>{'has(tri)': 1, 'has(find)': 1, 'has(diari)': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310512</th>\n",
       "      <td>The sole purpose of this update is to annoy @s...</td>\n",
       "      <td>{'has(the)': 1, 'has(sole)': 1, 'has(purpos)':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356794</th>\n",
       "      <td>@michael_luu Nice to see GM is using our tax d...</td>\n",
       "      <td>{'has(__user_michael_luu)': 1, 'has(nice)': 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196740</th>\n",
       "      <td>@wFranke @iamwun @stayingyoung @MsFitUniverse ...</td>\n",
       "      <td>{'has(__user_wfrank)': 1, 'has(__user_iamwun)'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622987</th>\n",
       "      <td>@LMRB no sun up north girl  oh btw ill be in m...</td>\n",
       "      <td>{'has(__user_lmrb)': 1, 'has(sun)': 1, 'has(no...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     tweet  \\\n",
       "989354   Trying to find my DIARY when i was in elementary    \n",
       "1310512  The sole purpose of this update is to annoy @s...   \n",
       "356794   @michael_luu Nice to see GM is using our tax d...   \n",
       "1196740  @wFranke @iamwun @stayingyoung @MsFitUniverse ...   \n",
       "622987   @LMRB no sun up north girl  oh btw ill be in m...   \n",
       "\n",
       "                                  processed_tweet_features  \n",
       "989354   {'has(tri)': 1, 'has(find)': 1, 'has(diari)': ...  \n",
       "1310512  {'has(the)': 1, 'has(sole)': 1, 'has(purpos)':...  \n",
       "356794   {'has(__user_michael_luu)': 1, 'has(nice)': 1,...  \n",
       "1196740  {'has(__user_wfrank)': 1, 'has(__user_iamwun)'...  \n",
       "622987   {'has(__user_lmrb)': 1, 'has(sun)': 1, 'has(no...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['processed_tweet_features'] = training_data.tweet.apply(extract_features)\n",
    "training_data[['tweet','processed_tweet_features']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Let's now use the processed tweet features to create a classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training-test Splitting\n",
    "\n",
    "To evaluate our approaches, we are going to split our data into train and validation. We will use the train to create the models and the validation to validate their performance. Once we have selected the best model (according to the accuracy on the validation set) we can use this model to predict our test set.\n",
    "\n",
    "In this way, test set will remain as unseen data for all the process: we are not going to make any decision based on the test error. Therefore, we can assume that the results on the test set will be the same that we will obtain when new unseen data appears in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = 4000\n",
    "train_tweets = [(tweet, sentiment) for tweet, sentiment in training_data[['tweet', 'sentiment']].values[:training_size]]\n",
    "validation_tweets  = [(tweet, sentiment) for tweet, sentiment in training_data[['tweet', 'sentiment']].values[training_size:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data for the classifier\n",
    "\n",
    "We have previously defined a feature extraction process, which we have wrapped into the `extract_features` function. By making use of the `nltk.classify.apply_features` function provided by NLTK, we will process the tweets and create the features that will be used for the classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the data processing and cleaning extraction methodologies\n",
    "v_train = nltk.classify.apply_features(extract_features,train_tweets)\n",
    "v_validation  = nltk.classify.apply_features(extract_features,validation_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the resultant object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the tweet =  Trying to find my DIARY when i was in elementary \n",
      " \n",
      "The following features has been created:\n",
      " \n",
      "{'has(tri)': 1, 'has(find)': 1, 'has(diari)': 1, 'has(when)': 1, 'has(wa)': 1, 'has(elementari)': 1, 'has(tri,find)': 1, 'has(find,diari)': 1, 'has(diari,when)': 1, 'has(when,wa)': 1, 'has(wa,elementari)': 1, 'has(tri,find,diari)': 1, 'has(find,diari,when)': 1, 'has(diari,when,wa)': 1, 'has(when,wa,elementari)': 1, 'neg_l(tri)': 0.0, 'neg_l(find)': 0.0, 'neg_l(diari)': 0.0, 'neg_l(when)': 0.0, 'neg_l(wa)': 0.0, 'neg_l(elementari)': 0.0, 'neg_r(tri)': 0.0, 'neg_r(find)': 0.0, 'neg_r(diari)': 0.0, 'neg_r(when)': 0.0, 'neg_r(wa)': 0.0, 'neg_r(elementari)': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"For the tweet = \", training_data.tweet.values[0])\n",
    "print(\" \")\n",
    "print(\"The following features has been created:\")\n",
    "print(\" \")\n",
    "print(v_train[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "We will start with a simple NaÃ¯ve Bayes Classifier. For a given tweet, if we need to find the label for it, we find the probabilities of all the labels, given that feature and then select the label with maximum probability.\n",
    "\n",
    "NLTK has its own implementation of Naive Bayes `nltk.classify.NaiveBayesClassifier`. If you prefer, you can use the Naive Bayes implementation in `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = nltk.classify.NaiveBayesClassifier\n",
    "nb_class = nb_classifier.train(v_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "Let's evaluate the accuracy of our model in our validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model =  0.738\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the model = \", nltk.classify.accuracy(nb_class, v_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "73 % of accuracy seems pretty good for the task.\n",
    "\n",
    "We can have a more detailed idea of the performance by taking a look to the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "         |   n   p |\n",
      "         |   e   o |\n",
      "         |   g   s |\n",
      "         |   a   i |\n",
      "         |   t   t |\n",
      "         |   i   i |\n",
      "         |   v   v |\n",
      "         |   e   e |\n",
      "---------+---------+\n",
      "negative |<381>128 |\n",
      "positive | 134<357>|\n",
      "---------+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build confusion matrix over validation set\n",
    "test_truth   = [s for (t,s) in v_validation]\n",
    "test_predict = [nb_class.classify(t) for (t,s) in v_validation]\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print()\n",
    "print(nltk.ConfusionMatrix( test_truth, test_predict ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most Representative Features\n",
    "\n",
    "The NLTK classifier object allows us to see the most representative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "              neg_l(sad) = 0.0            negati : positi =     27.9 : 1.0\n",
      "                has(sad) = 1              negati : positi =     22.5 : 1.0\n",
      "             has(welcom) = 1              positi : negati =     16.3 : 1.0\n",
      "           neg_r(welcom) = 0.0            positi : negati =     15.6 : 1.0\n",
      "           neg_l(welcom) = 0.0            positi : negati =     15.6 : 1.0\n",
      "               has(poor) = 1              negati : positi =     15.1 : 1.0\n",
      "             neg_l(poor) = 0.0            negati : positi =     14.4 : 1.0\n",
      "   has(__punc_excl,love) = 1              positi : negati =     14.2 : 1.0\n",
      "             neg_r(poor) = 0.0            negati : positi =     13.1 : 1.0\n",
      "      has(thank,for,the) = 1              positi : negati =     12.8 : 1.0\n",
      "            neg_l(enjoy) = 0.0            positi : negati =     12.7 : 1.0\n",
      "            neg_r(enjoy) = 0.0            positi : negati =     12.7 : 1.0\n",
      "               has(sick) = 1              negati : positi =     12.5 : 1.0\n",
      "             neg_l(sick) = 0.0            negati : positi =     12.1 : 1.0\n",
      "           has(love,the) = 1              positi : negati =     11.4 : 1.0\n",
      "            has(the,new) = 1              positi : negati =     11.4 : 1.0\n",
      "             neg_r(pain) = 0.0            negati : positi =     10.6 : 1.0\n",
      "               has(pain) = 1              negati : positi =     10.6 : 1.0\n",
      "            neg_r(studi) = 0.0            negati : positi =     10.6 : 1.0\n",
      "              has(studi) = 1              negati : positi =     10.6 : 1.0\n",
      "          has(thank,for) = 1              positi : negati =     10.6 : 1.0\n",
      "                has(woo) = 1              positi : negati =      9.4 : 1.0\n",
      "              neg_l(woo) = 0.0            positi : negati =      9.4 : 1.0\n",
      "            neg_l(sleep) = 0.9            negati : positi =      9.3 : 1.0\n",
      "             neg_l(pain) = 0.0            negati : positi =      9.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "nb_class.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "We have applied a thorough process to create features for our tweets. However, is it justified? Have we actually created a better representation of our data? To know that, we are going to create a baseline model that uses only the text in the tweets (with no features added).\n",
    "\n",
    "To that end we define a new extraction function that only extract the terms from the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_train_tweets = [(tweet.split(\" \"), sentiment) for tweet, sentiment in training_data[['tweet', 'sentiment']].values[:training_size]]\n",
    "baseline_validation_tweets  = [(tweet.split(\" \"), sentiment) for tweet, sentiment in training_data[['tweet', 'sentiment']].values[training_size:]]\n",
    "\n",
    "# Wrapper function for the extraction of features\n",
    "def extract_baseline_features(words):\n",
    "    \n",
    "    bag = {}\n",
    "    words_uni = [ 'has(%s)'% ug for ug in words ]\n",
    "    \n",
    "    for f in words_uni:\n",
    "        bag[f] = 1\n",
    "\n",
    "    return bag\n",
    "\n",
    "v_baseline_train = nltk.classify.apply_features(extract_baseline_features, baseline_train_tweets)\n",
    "v_baseline_validation = nltk.classify.apply_features(extract_baseline_features, baseline_validation_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit a new naive based classifier over this baseline representation and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_nb_classifier = nltk.classify.NaiveBayesClassifier\n",
    "baseline_nb_class = nb_classifier.train(v_baseline_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the baseline model =  0.69\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the baseline model = \", nltk.classify.accuracy(baseline_nb_class, v_baseline_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "         |   n   p |\n",
      "         |   e   o |\n",
      "         |   g   s |\n",
      "         |   a   i |\n",
      "         |   t   t |\n",
      "         |   i   i |\n",
      "         |   v   v |\n",
      "         |   e   e |\n",
      "---------+---------+\n",
      "negative |<351>158 |\n",
      "positive | 231<260>|\n",
      "---------+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build confusion matrix over validation set\n",
    "test_truth   = [s for (t,s) in v_baseline_validation]\n",
    "test_predict = [nb_class.classify(t) for (t,s) in v_baseline_validation]\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print()\n",
    "print(nltk.ConfusionMatrix( test_truth, test_predict ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, performance is significantly lower than that of the model using all the features we have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                has(sad) = 1              negati : positi =     24.7 : 1.0\n",
      "             has(Thanks) = 1              positi : negati =     19.8 : 1.0\n",
      "                has(Why) = 1              negati : positi =      9.3 : 1.0\n",
      "               has(lost) = 1              negati : positi =      9.3 : 1.0\n",
      "               has(shit) = 1              negati : positi =      9.3 : 1.0\n",
      "             has(missed) = 1              negati : positi =      9.2 : 1.0\n",
      "              has(sorry) = 1              negati : positi =      8.6 : 1.0\n",
      "            has(excited) = 1              positi : negati =      8.0 : 1.0\n",
      "               has(sick) = 1              negati : positi =      7.9 : 1.0\n",
      "               has(miss) = 1              negati : positi =      7.8 : 1.0\n",
      "               has(hate) = 1              negati : positi =      7.8 : 1.0\n",
      "              has(wanna) = 1              negati : positi =      7.4 : 1.0\n",
      "           has(headache) = 1              negati : positi =      7.4 : 1.0\n",
      "                has(Get) = 1              positi : negati =      7.3 : 1.0\n",
      "               has(isnt) = 1              negati : positi =      6.7 : 1.0\n",
      "             has(You're) = 1              positi : negati =      6.6 : 1.0\n",
      "               has(Good) = 1              positi : negati =      6.5 : 1.0\n",
      "           has(birthday) = 1              positi : negati =      6.5 : 1.0\n",
      "            has(fucking) = 1              negati : positi =      6.1 : 1.0\n",
      "              has(water) = 1              negati : positi =      6.1 : 1.0\n",
      "              has(sucks) = 1              negati : positi =      6.1 : 1.0\n",
      "               has(gone) = 1              negati : positi =      6.1 : 1.0\n",
      "             has(taking) = 1              negati : positi =      6.1 : 1.0\n",
      "             has(inside) = 1              negati : positi =      6.1 : 1.0\n",
      "                has(Too) = 1              negati : positi =      6.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Most Representative Features\n",
    "baseline_nb_class.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaxEnt Classifier\n",
    "\n",
    "Let's try a more sophisticated classifier to see if we can boost the classification performance. In particular we will apply a Maximum Entropy Classifier. \n",
    "\n",
    "\n",
    "This classifier works by finding a probability distribution that maximizes the likelihood of testable data. This probability function is parameterized by weight vector. The optimal value of which can be found out using the method of Lagrange multipliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (25 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.490\n",
      "             2          -0.69302        0.991\n",
      "             3          -0.69290        0.991\n",
      "             4          -0.69277        0.991\n",
      "             5          -0.69265        0.991\n",
      "             6          -0.69252        0.991\n",
      "             7          -0.69240        0.991\n",
      "             8          -0.69228        0.991\n",
      "             9          -0.69215        0.991\n",
      "            10          -0.69203        0.991\n",
      "            11          -0.69190        0.991\n",
      "            12          -0.69178        0.991\n",
      "            13          -0.69165        0.991\n",
      "            14          -0.69153        0.991\n",
      "            15          -0.69141        0.991\n",
      "            16          -0.69128        0.991\n",
      "            17          -0.69116        0.991\n",
      "            18          -0.69103        0.991\n",
      "            19          -0.69091        0.991\n",
      "            20          -0.69079        0.991\n",
      "            21          -0.69066        0.991\n",
      "            22          -0.69054        0.991\n",
      "            23          -0.69041        0.991\n",
      "            24          -0.69029        0.991\n",
      "         Final          -0.69017        0.991\n"
     ]
    }
   ],
   "source": [
    "max_ent_classifier = nltk.classify.MaxentClassifier\n",
    "max_ent_class = max_ent_classifier.train(v_train, algorithm='GIS', max_iter=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model =  0.743\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the model = \", nltk.classify.accuracy(max_ent_class, v_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "         |   n   p |\n",
      "         |   e   o |\n",
      "         |   g   s |\n",
      "         |   a   i |\n",
      "         |   t   t |\n",
      "         |   i   i |\n",
      "         |   v   v |\n",
      "         |   e   e |\n",
      "---------+---------+\n",
      "negative |<392>117 |\n",
      "positive | 140<351>|\n",
      "---------+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build confusion matrix over validation set\n",
    "test_truth   = [s for (t,s) in v_validation]\n",
    "test_predict = [max_ent_class.classify(t) for (t,s) in v_validation]\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print()\n",
    "print(nltk.ConfusionMatrix( test_truth, test_predict ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is similar than the one of Naive Bayes. If we review the most informative terms, we can see that both algorithms focus on similar features to perform the final classification; hence the similar performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -0.001 neg_l(sad)==0.0 and label is 'positive'\n",
      "  -0.001 has(sad)==1 and label is 'positive'\n",
      "  -0.001 has(welcom)==1 and label is 'negative'\n",
      "  -0.001 has(poor)==1 and label is 'positive'\n",
      "  -0.001 has(poor)_JJ==1 and label is 'positive'\n",
      "  -0.001 neg_r(welcom)==0.0 and label is 'negative'\n",
      "  -0.001 neg_l(welcom)==0.0 and label is 'negative'\n",
      "  -0.001 neg_l(poor)==0.0 and label is 'positive'\n",
      "  -0.001 has(__punc_excl,love)==1 and label is 'negative'\n",
      "  -0.001 neg_r(poor)==0.0 and label is 'positive'\n",
      "  -0.001 sent(sad)==-0.625 and label is 'positive'\n",
      "  -0.001 has(thank,for,the)==1 and label is 'negative'\n",
      "  -0.001 sent(Thanks)==0.125 and label is 'negative'\n",
      "  -0.001 has(sad)_JJ==1 and label is 'positive'\n",
      "  -0.001 has(love,the)==1 and label is 'negative'\n",
      "  -0.001 has(studi)==1 and label is 'positive'\n",
      "  -0.001 neg_r(studi)==0.0 and label is 'positive'\n",
      "  -0.001 has(wish)_NN==1 and label is 'positive'\n",
      "  -0.001 has(pain)==1 and label is 'positive'\n",
      "  -0.001 neg_r(pain)==0.0 and label is 'positive'\n",
      "  -0.001 has(the,new)==1 and label is 'negative'\n",
      "  -0.001 has(sick)==1 and label is 'positive'\n",
      "  -0.001 has(doesnt)_NN==1 and label is 'positive'\n",
      "  -0.001 neg_l(sick)==0.0 and label is 'positive'\n",
      "  -0.001 has(welcom)_NN==1 and label is 'negative'\n"
     ]
    }
   ],
   "source": [
    "max_ent_class.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentiWordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the theoretical session we presented some sentiment resources that could be used to enrich our dataset with external information.\n",
    "\n",
    "In particular, SentiWordNet provides a sentiment annotation for the WordNet synsets. We can add this sentiment annotation as new features to our dataset. \n",
    "\n",
    "In the following, we define a fuction that based on the words in the tweets and their POS tagging, find the sentiment annotation for the word_POS_TAG in SentiWordNet. We then add these values as new features in our dataset and use them to train a new MaxEnt Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "def penn_to_wn(tag):\n",
    "    \"\"\"\n",
    "    Convert between the PennTreebank tags to simple Wordnet tags\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    " \n",
    "\n",
    "def swn_polarity(text):\n",
    "    sentiment = 0.0\n",
    "    tokens_count = 0\n",
    "  \n",
    "    tagged_sentence = pos_tag(word_tokenize(text))\n",
    "    sentiment = {}\n",
    "    for word, tag in tagged_sentence:\n",
    "        \n",
    "        wn_tag = penn_to_wn(tag)\n",
    "        if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "            sentiment[\"sent(\"+word+\")\"] = 0.0\n",
    "            continue\n",
    "        \n",
    "        lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "        if not lemma:\n",
    "            sentiment[\"sent(\"+word+\")\"] = 0.0\n",
    "            continue\n",
    "\n",
    "        synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "        if not synsets:\n",
    "            sentiment[\"sent(\"+word+\")\"] = 0.0\n",
    "            continue\n",
    "\n",
    "        # Take the first sense, the most common\n",
    "        synset = synsets[0]\n",
    "        swn_synset = swn.senti_synset(synset.name())\n",
    "\n",
    "        sentiment[\"sent(\"+word+\")\"] = swn_synset.pos_score() - swn_synset.neg_score()\n",
    "        \n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sent(This)': 0.0,\n",
       " 'sent(is)': 0.0,\n",
       " 'sent(a)': 0.0,\n",
       " 'sent(text)': 0.0,\n",
       " 'sent(with)': 0.0,\n",
       " 'sent(good)': 0.75,\n",
       " 'sent(and)': 0.0,\n",
       " 'sent(very)': 0.0,\n",
       " 'sent(words)': 0.0,\n",
       " 'sent(bad)': -0.625,\n",
       " 'sent(stupid)': -0.75}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is a text with good and very good words and bad and stupid words\"\n",
    "swn_polarity(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This annotation provides a sentiment score (based on the SentiWordNet sentiment score) for each term in the tweets (-1 negative, 1 positive, 0 neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for the extraction of features + sentiment features\n",
    "def extract_features_with_sentiment(text):\n",
    "    features = {}\n",
    "    \n",
    "    words = processAll(text)\n",
    "    \n",
    "    sentiment_features = swn_polarity(text)\n",
    "    features.update(sentiment_features)\n",
    "    \n",
    "    word_features = get_word_features(words)\n",
    "    features.update( word_features )\n",
    "\n",
    "    negation_features = get_negation_features(words)\n",
    "    features.update( negation_features )\n",
    "        \n",
    "    pos_features = get_pos_features(words)\n",
    "    features.update( pos_features )\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the data processing and cleaning extraction methodologies\n",
    "v_train_sentiment = nltk.classify.apply_features(extract_features_with_sentiment,train_tweets)\n",
    "v_validation_sentiment  = nltk.classify.apply_features(extract_features_with_sentiment,validation_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (25 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.490\n",
      "             2          -0.69302        0.987\n",
      "             3          -0.69288        0.987\n",
      "             4          -0.69275        0.987\n",
      "             5          -0.69262        0.987\n",
      "             6          -0.69249        0.987\n",
      "             7          -0.69236        0.987\n",
      "             8          -0.69223        0.987\n",
      "             9          -0.69210        0.987\n",
      "            10          -0.69197        0.987\n",
      "            11          -0.69183        0.987\n",
      "            12          -0.69170        0.988\n",
      "            13          -0.69157        0.988\n",
      "            14          -0.69144        0.988\n",
      "            15          -0.69131        0.988\n",
      "            16          -0.69118        0.988\n",
      "            17          -0.69105        0.988\n",
      "            18          -0.69092        0.988\n",
      "            19          -0.69079        0.988\n",
      "            20          -0.69066        0.988\n",
      "            21          -0.69053        0.988\n",
      "            22          -0.69040        0.988\n",
      "            23          -0.69027        0.988\n",
      "            24          -0.69014        0.988\n",
      "         Final          -0.69001        0.988\n"
     ]
    }
   ],
   "source": [
    "# Train a new classfier with the sentiment features\n",
    "max_ent_classifier = nltk.classify.MaxentClassifier\n",
    "max_ent_class = max_ent_classifier.train(v_train_sentiment, algorithm='GIS', max_iter=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model =  0.737\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the model = \", nltk.classify.accuracy(max_ent_class, v_validation_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "         |   n   p |\n",
      "         |   e   o |\n",
      "         |   g   s |\n",
      "         |   a   i |\n",
      "         |   t   t |\n",
      "         |   i   i |\n",
      "         |   v   v |\n",
      "         |   e   e |\n",
      "---------+---------+\n",
      "negative |<401>108 |\n",
      "positive | 155<336>|\n",
      "---------+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build confusion matrix over validation set\n",
    "test_truth   = [s for (t,s) in v_validation_sentiment]\n",
    "test_predict = [max_ent_class.classify(t) for (t,s) in v_validation_sentiment]\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print()\n",
    "print(nltk.ConfusionMatrix( test_truth, test_predict ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have improve the accuracy of our classifier by including the sentiment information of the terms.\n",
    "\n",
    "If we take a look to the most informative features, we can find some sentiment-related features among them. For instance, `sad` has a negative implication, codified by the feature: `sent(sad)==-0.625`, which is highly informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -0.001 neg_l(sad)==0.0 and label is 'positive'\n",
      "  -0.001 has(sad)==1 and label is 'positive'\n",
      "  -0.001 has(welcom)==1 and label is 'negative'\n",
      "  -0.001 has(poor)==1 and label is 'positive'\n",
      "  -0.001 has(poor)_JJ==1 and label is 'positive'\n",
      "  -0.001 neg_r(welcom)==0.0 and label is 'negative'\n",
      "  -0.001 neg_l(welcom)==0.0 and label is 'negative'\n",
      "  -0.001 neg_l(poor)==0.0 and label is 'positive'\n",
      "  -0.001 has(__punc_excl,love)==1 and label is 'negative'\n",
      "  -0.001 neg_r(poor)==0.0 and label is 'positive'\n",
      "  -0.001 sent(sad)==-0.625 and label is 'positive'\n",
      "  -0.001 has(thank,for,the)==1 and label is 'negative'\n",
      "  -0.001 sent(Thanks)==0.125 and label is 'negative'\n",
      "  -0.001 has(sad)_JJ==1 and label is 'positive'\n",
      "  -0.001 has(love,the)==1 and label is 'negative'\n",
      "  -0.001 has(studi)==1 and label is 'positive'\n",
      "  -0.001 neg_r(studi)==0.0 and label is 'positive'\n",
      "  -0.001 has(wish)_NN==1 and label is 'positive'\n",
      "  -0.001 has(pain)==1 and label is 'positive'\n",
      "  -0.001 neg_r(pain)==0.0 and label is 'positive'\n",
      "  -0.001 has(the,new)==1 and label is 'negative'\n",
      "  -0.001 has(sick)==1 and label is 'positive'\n",
      "  -0.001 has(doesnt)_NN==1 and label is 'positive'\n",
      "  -0.001 neg_l(sick)==0.0 and label is 'positive'\n",
      "  -0.001 has(welcom)_NN==1 and label is 'negative'\n"
     ]
    }
   ],
   "source": [
    "max_ent_class.show_most_informative_features(25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
